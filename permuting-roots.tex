\chapter{Permuting roots}
\epigraph[author={Hermann Weyl}, etc={On \'Evariste Galois's letter, written the night before Galois died in a pistol duel. Galois's letter is about permutations of roots of polynomials.}]{This letter, if judged by the novelty and profundity of ideas it contains, is perhaps the most substantial piece of writing in the whole literature of mankind.}\SubIndex{Weyl, Hermann}

\section{Vieta's formulas}

For now, we work over any field.
Which polynomials have which roots?
How are the coefficients related to the roots?
\begin{example}
To get a quadratic polynomial \(p(x)\) to have roots at \(3\) and \(7\), we need it to have \(x-3\) and \(x-7\) as factors.
So it has to be 
\begin{align*}
p(x)
&=
a(x-3)(x-7),
\\
&=
a\pr{x^2-7x-3x+(-3)(-7)},
\\
&=
a\pr{x^2-(3+7)x+3 \cdot 7}.
\end{align*}
\end{example}

A polynomial is \emph{monic}\define{monic polynomial}\define{polynomial!monic} if its leading coefficient is \(1\).
We can make any polynomial monic by dividing off the leading term:
\[
3x^2-4x+1 = 3\pr{x^2-\frac{4}{3}x + \frac{1}{3}}.
\]
The only monic quadratic polynomial \(p(x)\) with roots \(x=3\) and \(x=7\) is
\[
p(x) = x^2-(3+7)x+3 \cdot 7.
\]
The only quadratic monic polynomial \(p(x)\) with roots \(x=r\) and \(x=s\), by the same steps, must be
\[
p(x) = x^2-(r+s)x+rs.
\]
By the same steps, the only cubic monic polynomial \(q(x)\) with roots at \(x=r\), \(x=s\) and \(x=t\) is
\[
q(x)=(x-r)(x-s)(x-t).
\]
If we multiply it all out, we get
\[
q(x)=x^3-(r+s+t)x^2+(rs+rt+st)x-rst.
\]
Ignoring the minus signs, the coefficients are
\begin{align*}
&1 \\
&r+s+t \\
&rs+rt+st \\
&rst
\end{align*}
There is some pattern: 
\begin{enumerate}
\item
There is a minus sign, turning on and off like a light switch, each time we write down a term.
\item
Each coefficient is a sum of products of roots, taken in all possible ways with a fixed number of roots.
\end{enumerate}

\begin{proposition}[Vieta's formula (Viet\'e)]\SubIndex{Viet\'e, Francois}
If a monic polynomial \(p(x)\) splits into a product of linear factors, say
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
then the numbers \(r_1, r_2, \dots, r_n\) are the roots of \(p(x)\), and the coefficients of \(p(x)\), say
\[
p(x) = x^n - a_1 x^{n-1} + \dots \pm  a_{n-1} x \pm a_n,
\]
(with signs switching each term in front of the \(a_j\) coefficients) are computed from the roots by
\begin{align*}
a_1 &= r_1 + r_2 + \dots + r_n, \\
a_2 &= \sum_{i < j} r_i r_j, \\
& \vdotswithin{=} \\
a_i &= \sum_{j_1 < j_2 < \dots < j_i} r_{j_1} r_{j_2} \dots r_{j_i}, \\
& \vdotswithin{=} \\
a_{n-1} &= r_1 r_2 \dots r_{n-1} + r_1 r_2 \dots r_{n-2} r_n + \dots + r_2 r_3 \dots r_n, \\
a_n &= r_1 r_2 \dots r_n.
\end{align*}
\end{proposition}
The \emph{elementary symmetric polynomials} are the polynomials \(e_1, \dots, e_n\)\Notation{ej}{e_j}{elementary symmetric polynomials} of variables \(t_1, t_2, \dots, t_n\), given by
\begin{align*}
e_1\of{t_1,t_2,\dots,t_n} &= t_1 + t_2 + \dots + t_n, \\
e_2\of{t_1,t_2,\dots,t_n} &= t_1 t_2 + t_1 t_3 + \dots + t_{n-1} t_n, \\
                          &= \sum_{i < j} t_i t_j, \\
                          & \vdotswithin{=} \\
e_i\of{t_1,t_2,\dots,t_n} &= \sum_{j_1 < j_2 < \dots < j_i} t_{j_1} t_{j_2} \dots t_{j_i}, \\
                          & \vdotswithin{=} \\
e_{n-1}\of{t_1,t_2,\dots,t_n} &= t_1 t_2 \dots t_{n-1} + t_1 t_2 \dots t_{n-2} t_n + \dots + t_2 t_3 \dots t_n, \\
e_n\of{t_1,t_2,\dots,t_n} &= t_1 t_2 \dots t_n.
\end{align*}
So we can restate our proposition as:
\begin{proposition}[Vieta's formula]\label{proposition:vieta}
If a monic polynomial \(p(x)\) splits into a product of linear factors, say
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
then the numbers \(r_1, r_2, \dots, r_n\) are the roots of \(p(x)\), and the coefficients of \(p(x)\):
\[
p(x) = x^n - e_1 x^{n-1} + \dots (-1)^{n-1}  e_{n-1} x + (-1)^n e_n,
\]
are the elementary symmetric polynomials 
\[
e_i=e_i\of{r_1,r_2,\dots,r_n}
\]
of the roots \(r_1, r_2, \dots, r_n\).
\end{proposition}

\begin{proof}
We can see this immediately for linear polynomials: \(p(x)=x-r\), and we checked it above for quadratic and cubic ones.
It is convenient to rewrite the elementary symmetric polynomials as 
\[
e_i\of{r_1,r_2,\dots,r_n}
=
\sum r_1^{b_1} r_2^{b_2} \dots r_n^{b_n}
\]
where the sum is over all choices of numbers \(b_1, b_2, \dots, b_n\) so that
\begin{enumerate}
\item
each of these \(b_j\) is either \(0\) or \(1\) and
\item
so that altogether 
\[
b_1 + b_2 + b_3 + \dots + b_n = i,
\]
\end{enumerate}
or in other words, ``turn on'' \(i\) of the roots, and turn off the rest, and multiply out the ones that are turned on, and then sum over all choices of which to turn on.

If we expand out the product
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
we do so by pick whether to multiply with \(x\) or with \(-r_1\) from the first factor, and then whether to multiply with \(x\) or with \(-r_2\) from the second, and so on, and we add over all of these choices.
Each choice write as \(b_1=0\) if we pick to multiply in the \(x\), but \(b_1=1\) if we pick to multiply in the \(-r_1\), and so on:
\begin{align*}
p(x) 
&= 
\sum_{b_1,\dots,b_n} \pr{-r_1}^{b_1} \pr{-r_2}^{b_2} \dots \pr{-r_n}^{b_n}
x^{\pr{1-b_1} + \pr{1-b_2} + \dots + \pr{1-b_n}},
\\
&=
\sum_j
\sum_{b_1+\dots+b_n=j} \pr{-r_1}^{b_1} \pr{-r_2}^{b_2} \dots \pr{-r_n}^{b_n}
x^{n-j}.
\end{align*}
\end{proof}

\section{Symmetric polynomials}
A polynomial \(f\left(t_1,t_2,\dots,t_n\right)\) is \emph{symmetric}\define{symmetric!polynomial}\define{polynomial!symmetri c} if it is unchanged by permuting the variables \(t_1, t_2, \dots, t_n\).
\begin{example}
\(t_1+t_2+\dots+t_n\) is symmetric.
\end{example}
For any numbers or variables \(t=\pr{t_1, t_2, \dots, t_n}\) let
\[
P_t(x)\defeq\left(x-t_1\right)\left(x-t_2\right) \dots \left(x-t_n\right).
\]
Clearly the roots of \(P_t(x)\) are precisely the entries of the vector
\(t\).
Let \(e(t)\defeq\pr{e_1(t),e_2(t),\dots,e_n(t)}\), so that \(e\) maps vectors to vectors (with entries in our field).

Recall that a field is \emph{algebraically closed} if every polynomial with coefficients in that field splits into a product of linear factors with coefficients in that field.
Recall the fundamental theorem of algebra (theorem~\vref{theorem:FTA}): the field of complex numbers \(\C{}\) is algebraically closed.
\begin{lemma}
For each vector \(c\) with entries in a field, there is a vector \(t\), with entries in a finite degree extension field, so that \(e(t)=c\).
In particular, there is no relation between the elementary symmetric polynomials.
Over an infinite field, there is no nontrivial polynomial equation satisfied by the elementary symmetric polynomials.
\end{lemma}
\begin{proof}
Let \(t_1, t_2, \dots, t_n\) be the roots of the polynomial
\[
P(x) = x^n - c_1 x^{n-1} + c_2 x^{n-2} + \dots + (-1)^n c_n.
\]
By proposition~\vref{proposition:vieta}, \(P_t(x)\) has coefficients precisely the same as those of \(P(x)\), i.e. \(P_t(x)=P(x)\).
Any relation \(R(e(t))=0\) between the elementary symmetric polynomials must be satisfied at all points \(c\), so is trivial for any function \(R\).
For a polynomial \(R\), which we don't think of as a function, over an infinite field, the expression \(R(e(t))\) vanishing forces \(R(c)\) to vanish for each \(c\).
Varying any one component of \(c\), \(R(c)\) is a polynomial function of that component with infinitely many roots, so is zero in that component for all other components fixed, so the zero polynomial by induction.
\end{proof}
\begin{lemma}
Over any field, the entries of two vectors \(s\) and \(t\) are permutations of one another just when \(e(s)=e(t)\), i.e. just when the elementary symmetric polynomials take the same values at \(s\) and at \(t\).
\end{lemma}
\begin{proof}
The roots of \(P_s(x)\) and \(P_t(x)\) are the same numbers.
\end{proof}
We want to see that every symmetric polynomial \(f\of{t_1,t_2,\dots,t_n}\) has the form \(f(t)=h(e(t))\), for a unique polynomial \(h\), and conversely if \(h\) is any function at all, then \(f(t)=h(e(t))\) determines a symmetric polynomial.
We want a recipe to write down each symmetric polynomial in terms of the elemenary symmetric polynomials, to find this mysterious \(h\).
\begin{example}
Take the function 
\[
f=3x^2yz+3xy^2z+3xyz^2+5xy+5xz+5yz.
\]
Clearly the terms with the 5's look like an elementary symmetric function:
\[
f=3x^2yz+3xy^2z+3xyz^2+5e_2.
\]
(We write \(e_2\) to mean the function \(e_2(x,y,z)=xy+xz+yz\), the 2nd elementary symmetric function.)
But what about the terms with the 3's?
Factor them all together as much as we can:
\[
f=3xyz(x+y+z)+5e_2.
\]
Then it is clear:
\[
f=3e_3e_1+5e_2.
\]
\end{example}

If \(a=\pr{a_1,a_2,\dots,a_n}\), write \(t^a\)\Notation{xa}{x^a}{multivariable exponent} to mean
\(t_1^{a_1} t_2^{a_2} \dots t_n^{a_n}\). 
Order terms by ``alphabetical'' order, also called \emph{weight}\define{weight}: order monomials by the order in \(t_1\); if two monomials have the same order in \(t_1\), break the tie by looking at the order in \(t_2\), and so on.
\begin{example}
The term
\[
t_1^5 t_2^3 
\]
has higher weight than any of
\[
t_1^3 t_2^5, t_1^5 t_2^2, 1, \text{ and } t_2^{1000}.
\]
\end{example}
\begin{example}
Showing the highest order term, and writing dots to indicate lower order terms,
\begin{align*}
e_1(t)&=t_1 + \dots, \\
e_2(t)&=t_1t_2 + \dots, \\
& \vdotswithin{=} \\
e_j(t)&=t_1 t_2 \dots t_j + \dots \\
& \vdotswithin{=} \\
e_n(t)&=t_1 t_2 \dots t_n.
\end{align*}
\end{example}

If a symmetric polynomial contains a term, then it also contains every term obtained by permuting the variables:
\[
t_1^3 t_2^5 + t_1^5 t_2^3.
\]
\begin{example}
It is somehow easier to read \(x,y,z\) than \(t_1,t_2,t_3\), so take a symmetric polynomial 
\[
f(x,y,z)=6 x^{16} y^9 z^7 + \dots
\]
where we only write out the highest weight term.
In such notation, 
\begin{align*}
e_1(x,y,z)&=x+y+z=x+\dots, \\
e_2(x,y,z)&=xy+xz+yz=xz + \dots, \\
e_3(x,y,z)&=xyz.
\end{align*}
Imagine a term in \(f\): if the highest power in that term is in \(y\) or \(z\), permute the variables to get that highest power into \(x\), giving a higher weight term in \(f\).
So the highest weight term contains the variables \(x, y, z\), with highest power in \(x\) and successively lower powers in each of \(y,z\).
By the same reasoning, inside the highest weight term, each of \(x,y,z\) appears to at least to the power of the \(z\) variable: in this case, at least to a power of \(7\).
Factor out \(7\) powers of each variable; we underline that factored out part so you can see it:
\[
f=6 x^9 y^2 \underline{\pr{x y z}^7} + \dots
\]
In the remaining factors, there are now no \(z\) variables, and each variable appears at least to a power of \(2\), so we factor out 2 of each:
\[
f=6 x^7 \underline{\pr{xy}^2} \pr{xyz}^7 + \dots
\]
So finally it is clear that \(f\) has the same highest weight as
\(
6 e_1^7 e_2^2 e_3^7.
\)
Hence 
\[
f = 6 e_1^7 e_2^2 e_3^7 + \dots
\]
up to terms of lower weight.
\end{example}

%If we pick integers \(d_1, d_2, \dots, d_n \ge 0\),
%\begin{align*}
%e_1(t)^{d_1} e_2(t)^{d_2} \dots e_n(t)^{d_n}
%&=
%t_1^{d_1} \pr{t_1t_2}^{d_2} \dots \pr{t_1t_2\dots t_n}^{d_n} + \dots,
%\\
%&=
%t_1^{d_1+d_2+\dots+d_n} t_2^{d_2+d_3+\dots+d_n} \dots 
%t_n^{d_n} + \dots.
%\end{align*}
%Write this more neatly as
%\[
%e(t)^d = t^a+\dots
%\]
%where
%\begin{align*}
%a_1&\defeq d_1+d_2+d_3+\dots+d_n, \\ 
%a_2&\defeq d_2+d_3+\dots+d_n, \\
%a_3&\defeq d_3+\dots+d_n
%& \vdotswithin{\defeq} \\
%a_n&\defeq d_n.
%\end{align*}
%
%On the other hand, given a decreasing sequence of nonnegative integers 
%\[
%a=\pr{a_1,a_2,\dots,a_n}
%\]
%calculate \(d\) by
%\begin{align*}
%d_1&\defeq a_1-a_2, \\
%d_2&\defeq a_2-a_3, \\
%& \vdotswithin{\defeq} \\
%d_n&\defeq a_n
%\end{align*}
%and then \(e(t)^d=t^a+\dots\).
%So for each decreasing sequence \(a=\pr{a_1,a_2,\dots,a_n}\), we have \(t^a=e(t)^d+\dots\).

\begin{theorem}\label{theorem:symmetric.polynomials.algebra}
Every symmetric polynomial \(f\) has exactly one expression as a polynomial
in the elementary symmetric polynomials.
If \(f\) has integer coefficients, then \(f\) is an integer coefficient polynomial of the elementary symmetric polynomials. Similarly if \(f\) has coefficients in any field, then \(f\) is a polynomial of the elementary symmetric polynomials, with coefficients in that same field.
\end{theorem}
\begin{proof}
Write the highest weight term as some coefficient times \(t_1^{a_1} t_2^{a_2} \dots t_n^{a_n}\).
For simplicity, write this as \(t^a\).
If \(a_1\) is smaller than \(a_2\), permute them to get a higher weight term, a contradiction, so \(a_1 \ge a_2\).
In the same way, \(a_1 \ge a_2 \ge a_3 \ge \dots \ge a_n\).
It might be that various of these \(a_i\) are zero; instead we can write the term as \(t_1^{a_1} \dots t_k^{a_k}\), with all positive powers \(a_1\ge a_2 \ge \dots \ge a_k>0\).
Let 
\[
b_1\defeq a_1-a_k, \dots b_{k-1}\defeq a_{k-1}-a_k.
\]
Factor out:
\[
t^a=t^b (t_1 \dots t_k)^{a_k}.
\]
This is the highest weight term in
\[
t^b e_k^{a_k}.
\]
For simplicity, write expressions like \(e_1^{d_1} \dots e_n^{d_n}\) as \(e^d\).
By induction, we can then find some expression \(e^d\) with highest weight term \(t^b\), so
\[
t^a=t^b e_k^{a_k} + \dots = e^d e_k^{a_k} + \dots
\]
where \(\dots\) means lower weight terms.
By induction, we can somehow write the lower weight terms as polynomials in the elementary symmetric functions.
\end{proof}
\begin{example}
The sum of squares of two variables is symmetric: 
\[
x^2+y^2=\pr{x+y}^2-2xy.
\] 
To compute out these expressions: \(f(x,y)=x^2+y^2\) has highest term \(x^2\). 
The polynomials \(e_1(x,y)=x+y\) and \(e_2(x,y)=xy\) have highest terms \(x\) and \(xy\).
So we subtract off \(e_1(x,y)^2\) from \(f(x,y)\), and find \(f(x,y)-e_1(x,y)^2=-2xy=-2e_2(x,y)\).
\end{example}

\begin{problem}{permuting.roots:express.in.s}
Express each of the following polynomials as polynomials in the elementary symmetric polynomials:
\begin{enumerate}
\item \(x^3y^3z + x^3yz^3+xy^3z^3\)
\item \(4xyz^3+4xzy^3+4yzx^3\)
\item \(x^4y^4z^4\)
\item \(xyz+x^2y^2z^2\)
\end{enumerate}
\end{problem}
\begin{answer}{permuting.roots:express.in.s}
\begin{enumerate}
\item
\begin{align*}
x^3y^3z + x^3yz^3+xy^3z^3
&=
(xyz)(x^2y^2+x^2z^2+y^2z^2),
\\
&=e_3(x^2y^2+\dots),
\\
&=
e_3(e_2^2+\dots).
\end{align*}
But \(e_2=xy+xz+yz\), so 
\[
e_2^2=x^2y^2+x^2z^2+y^2z^2+2x^2yz+2xy^2z+2xyz^2,
\]
so
\begin{align*}
x^3y^3z + x^3yz^3+xy^3z^3
&=
e_3(e_2^2-[2x^2yz+2xy^2z+2xyz^2]),
\\
&=
e_3(e_2^2-2e_3[x+y+z]),
\\
&=
e_3(e_2^2-2e_3e_1)
\end{align*}
\item \(4xyz^3+4xzy^3+4yzx^3=4e_3(e_1^2-2e_2)\)
\item \(x^4y^4z^4=e_3^4\)
\item \(xyz+x^2y^2z^2=e_3\pr{1+e_3}\)
\end{enumerate}
\end{answer}

\section{Sage}
Sage can compute with elementary symmetric polynomials.
It writes them in a strange notation.
The polynomials we have denoted by \(e_3\) sage denotes by \verb!e[3]!.
More strangely, sage denotes \(e_1 e_2^3 e_5^2\) as \verb!e[5,5,2,2,2,1]! which is the same in sage as \verb!e[2]^3*e[1]*e[5]^2!.
You can't write \verb!e[1,2,2,2,5,5]!; you have to write it as \verb!e[5,5,2,2,2,1]!: the indices have to decrease.
Set up the required rings:
\begin{sageblock}
P.<w,x,y,z>=PolynomialRing(QQ)
S=SymmetricFunctions(QQ)
e=S.e()
\end{sageblock}
This creates a ring \(P=\Q{}[w,x,y,z]\), and then lets \(S\) be the ring of symmetric polynomials.
The last (mysterious) line sets up the object \(e\) to be the elementary symmetric polynomials of the ring \(S\).
We can then define a polynomial in our variables:
\begin{sageblock}
f = w^2+x^2+y^2+z^2+w*x+w*y+w*z+x*y+x*z+y*z
e.from_polynomial(f)
\end{sageblock}
which prints out \(\sage{e.from_polynomial(f)}\), the expression of \(f\) in terms of elementary symmetric polynomials.
To expand out a symmetric polynomial into \(w,x,y,z\) variables:
\begin{sageblock}
q = e[2,1]+e[3]
q.expand(4,alphabet=['w','x','y','z'])
\end{sageblock}
prints out 
\[
\sage{q.expand(4,alphabet=['w','x','y','z'])}
\]


\section{Sums of powers}
Define \(p_j(t)=t_1^j+t_2^j+\dots+t_n^j\),\Notation{pj}{p_j}{sum of powers polynomials} the sums of powers.
\begin{lemma}[Isaac Newton\SubIndex{Newton, Isaac}]
The sums of powers are related to the elementary symmetric polynomials by
\begin{align*}
0 &= e_1 - p_1, \\
0 &= 2 \, e_2 - p_1 \, e_1 + p_2, \\ 
& \vdotswithin{=} \\
0 &= k \, e_k - p_1 \, e_{k-1} + p_2 \, e_{k-2} - \dots + (-1)^{k-1} p_{k-1} \, e_1
+ (-1)^k p_k,
\end{align*}
\end{lemma}
Using these equations, we can write the elementary symmetric polynomials inductively in terms of the sums of powers, or vice versa.
\begin{proof}
Let's write \(t^{({\ell})}\) for \(t\) with the \({\ell}\)\textsuperscript{th} entry removed, so if \(t\) is a vector with \(n\) entries, then \(t^{({\ell})}\) is a vector with \(n-1\) entries.
\begin{align*}
p_j e_{k-j}
&=
\sum_{\ell} t_{\ell}^j
\sum_{i_1 < i_2 < \dots < i_{k-j}} t_{i_1} t_{i_2} \dots t_{i_{k-j}}
\\
\intertext{Either we can't pull a $t_{\ell}$ factor out of the second sum, or we can:}
&=
\sum_{\ell} t_{\ell}^j
\sum^{i_1, i_2, \dots \ne {\ell}}_{i_1 < i_2 < \dots < i_{k-j}}
t_{i_1} t_{i_2} \dots t_{i_{k-j}}
+
\sum_{\ell} t_{\ell}^{j+1}
\sum^{i_1, i_2, \dots \ne {\ell}}_{i_1 < i_2 < \dots < i_{k-j-1}}
t_{i_1} t_{i_2} \dots t_{i_{k-j-1}}
\\
&=
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{({\ell})}\right)
+
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right).
\end{align*}
Putting in successive terms of our sum,
\begin{align*}
p_j e_{k-j} - p_{j+1} e_{k-j-1}
=&
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{(\ell)}\right)
+
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right)
\\
-&
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right)
-
\sum_{\ell} t_{\ell}^{j+2} e_{k-j-2}\left(t^{({\ell})}\right)
\\
=&
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{({\ell})}\right)
-
\sum_{\ell} t_{\ell}^{j+2} e_{k-j-2}\left(t^{({\ell})}\right).
\end{align*}
Hence the sum collapses to
\begin{align*}
p_1 e_k - p_2 e_{k-1} + \dots + (-1)^{k-1} p_{k-1} e_1
&=
\sum_{\ell} t_{\ell} e_{k-1}\left(t^{({\ell})}\right)
+(-1)^{k-1}
\sum_{\ell} t_{\ell}^k \cdot e_0\left(t^{(\ell)}\right)
\\
&=
k \, e_k + (-1)^{k-1} p_k.
\end{align*}
\end{proof}
\begin{proposition}
Every symmetric polynomial is a polynomial in the sums of powers. If the coefficients of the symmetric polynomial lie in a field, then it is a polynomial function of the sums of powers over that same field.
\end{proposition}
\begin{proof}
We can solve recursively for the sums of powers in terms of the elementary symmetric polynomials and conversely.
\end{proof}

\section{The invariants of a square matrix}\label{pg:linear.invariants}
Over any field, a polynomial \(f(A)\) in the entries of a square matrix \(A\), with coefficients in the field, is \emph{invariant}\define{invariant!of square matrix} if \(f\left(FAF^{-1}\right)=f(A)\) for any invertible matrix \(F\) with coefficients in the field.
An invariant is independent of change of basis: if \(T \colon V \to V\) is a linear map on an \(n\)-dimensional vector space, we can define the value \(f(T)\) of any invariant \(f\) of \(n \times n\) matrices, by letting \(f(T)\defeq f(A)\) where \(A\) is the matrix associated to \(T\) in some basis of \(V\).
\begin{example}
For any \(n \times n\) matrix \(A\), write
\[
\det(A-\lambda I)=\chi_A(\lambda)=
e_n(A) - e_{n-1}(A) \lambda + e_{n-2}(A) \lambda^2 + \dots + (-1)^n \lambda^n.
\]
The expressions \(e_1(A), e_2(A), \dots, e_n(A)\) are invariants, while \(\chi_A(\lambda)\) is the \emph{characteristic polynomial}\define{characteristic polynomial} of \(A\).
\end{example}
\begin{example}\label{example:Powers}
If we write the trace of a matrix \(A\) as \(\tr{A}\), then the functions
\[
p_k(A) = \tr\of{A^k}\SubIndex{trace}
\]
are invariants.
\end{example}
\begin{problem}{SymmetricFunctions:Two}
If \(A\) is diagonal, say
\[
A =
\begin{pmatrix}
t_1 \\
& t_2 \\
& & \ddots \\
& & & t_n
\end{pmatrix},
\]
then prove that \(e_j(A)=e_j\left(t_1,t_2,\dots,t_n\right)\), the elementary symmetric polynomials of the eigenvalues.
\end{problem}
\begin{answer}{SymmetricFunctions:Two}
\begin{align*}
\chi_A{\lambda}
&=
\left(t_1-\lambda\right)\left(t_2-\lambda\right)
\dots \left(t_n-\lambda\right)\\
&=(-1)^n P_{t}\left(\lambda\right)\\
&=
e_n(t) - e_{n-1}(t) \lambda + e_{n-2}(t) \lambda^2 + \dots + (-1)^n \lambda^n.
\end{align*}
\end{answer}
\begin{problem}{permuting.roots:diag.symmetric}
Generalize the previous exercise to \(A\) diagonalizable.
\end{problem}
\begin{problem}{permuting.roots:inverse.Cramer}
Prove that the entries of \(A^{-1}\) are rational functions of the entries of the square matrix \(A\), over any field.
\end{problem}
\begin{answer}{permuting.roots:inverse.Cramer}
Cramer's rule says that the inverse \(A^{-1}\) has entries
\[
A^{-1}_{ij} = \frac{(-1)^{i+j}\det (A \text{ with row \(j\) and column \(i\) deleted})}{\det A},
\]
so rational functions of the entries of \(A\).
\end{answer}

\begin{example}
Let \(\Delta_A\) be the discriminant of the characteristic polynomial of \(A\): \(\Delta_A \defeq \Delta_{\chi_A}\).
The map \(A \mapsto \Delta_A\) is also a polynomial invariant of \(A\), as the coefficients of the characteristic polynomial are.
\end{example}
\begin{problem}{SymmetricFunctions:discriminant}
Take a square matrix \(A\).
Suppose that the characteristic polynomial of \(A\) splits into linear factors.
Prove that \(\Delta_A = 0\) just when \(A\) has an eigenvalue of multiplicity two or more.
\end{problem}
\begin{theorem}
Every invariant polynomial of square matrices over an infinite field has exactly one expression as a polynomial in the elementary symmetric polynomials of the eigenvalues. 
\end{theorem}
We can replace the elementary symmetric polynomials of the eigenvalues by the sums of powers of the eigenvalues.
\begin{proof}
Take an invariant polynomial \(f(A)\).
Every invariant polynomial \(f(A)\) determines an invariant polynomial \(f(t)\)
by setting
\[
A =
\begin{pmatrix}
t_1 \\
& t_2 \\
& & \ddots \\
& & & t_n
\end{pmatrix}.
\]
Taking \(F\) any permutation matrix, invariance tells us that \(f\left(FAF^{-1}\right)=f(A)\). 
But \(f\left(FAF^{-1}\right)\) is given by applying the associated permutation to the entries of \(t\).
Therefore \(f(t)\) is a symmetric function. 
Therefore \(f(t)=h(e(t))\), for some polynomial \(h\); so \(f(A)=h(e(A))\) for  diagonal matrices. 
Replace \(f\) by \(f(A)-h(e(A))\) to arrange that \(f(A)=0\) on all diagonal matrices \(A\).
By invariance, \(f(A)=0\) on all diagonalizable matrices. 

The equation \(f(FAF^{-1})=f(A)\) holds for all \(F\) over our field.
Imagine that \(F\) has abstract variables as entries.
The difference \(f(FAF^{-1})-f(A)\) is a rational function in the entries of \(F\).
Its numerator vanishes for any choice of values of those abstract variables, and so vanishes.
Hence \(f(FAF^{-1})=f(A)\) for \(F\) with abstract variable entries.
In particular, if our field lies in a larger field, then \(f\) remains invariant over the larger field, because we can plug into the entries of \(F\) the values in the larger field.

Recall that a matrix has an eigenvector for each eigenvalue, so if there are \(n\) distinct eigenvalues, then \(A\) is diagonalizable.
This occurs just when the characteristic polynomial \(\chi_A(\lambda)\) splits into distinct linear factors, and so \(\Delta_A \ne 0\).
Conversely, if \(\Delta_A \ne 0\) and \(\chi_A(\lambda)\) splits into linear factors, then they are distinct, and so \(f(A)=0\).
By theorem~\vref{theorem:splitting.field}, we can sit our field into a larger field in which the characteristic polynomial \(\chi_A(\lambda)\) splits into linear factors, and therefore if \(\Delta_A\ne 0\) then \(f(A)=0\).

Pick any matrix \(A_0\) whose eigenvalues are all distinct.
In particular, \(\Delta_{A_0} \ne 0\).
Take any matrix \(A_1\).
For an abstract variable \(t\), let \(A_t \defeq (1-t)A_0+tA_1\).
Since \(\Delta_{A_t}\) is a polynomial, not vanishing at \(t=0\), it is a nonzero polynomial in \(t\).
So \(\Delta_{A_t}\ne 0\) except for finitely many \(t\).
Hence \(f(A_t)=0\) except for finitely many \(t\).
But \(f(A)\) is a polynomial, so vanishes for all \(t\).
\end{proof}

\begin{example}
The function \(f(A)=e_j\left(\left|\lambda_1\right|,\left|\lambda_2\right|,\dots,\left|\lambda_n\right|\right)\), where \(A\) has eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_n\), is a continuous invariant function of a real matrix \(A\), and is \emph{not} a polynomial in \(\lambda_1, \lambda_2, \dots, \lambda_n\).
\end{example}

\section{Resultants and permutations}
\begin{example}
If \(b(x,y)=x^2+y^2-1\) and \(c(x,y)=x^2+(y-1)^2-1\), how can we find the points \((x,y)\) at which both \(b(x,y)=0\) and \(c(x,y)=0\)? 
\begin{center}
\inputinexample{degenerate-resultant-0}
\end{center}
Think of \(b(x,y)\) and \(c(x,y)\) as polynomials in \(x\), with coefficients rational functions of \(y\).
Then the resultant \(r(y)\) of \(b(x,y),c(x,y)\) is a rational function of \(y\).
Compute it: \(r(y)=(2y-1)^2\) vanishes at \(y=1/2\).
So there is a common factor there:
\[
b(x,1/2)=c(x,1/2)=\pr{x-\frac{\sqrt{3}}{2}}\pr{x+\frac{\sqrt{3}}{2}}.
\]
The common factor came from two common roots.
\end{example}

In chapter~\ref{chapter:resultants}, we saw that for any two polynomials split into linear factors
\begin{align*}
b(x) &= \pr{x-\beta_1} \pr{x-\beta_2} \dots \pr{x-\beta_m}, \\
c(x) &= \pr{x-\gamma_1} \pr{x-\gamma_2} \dots \pr{x-\gamma_n}, \\
\end{align*}
the resultant is
\begin{align*}
\resultant{b}{c} 
=
\pr{\gamma_1-\beta_1}\pr{\gamma_1-\beta_2}\dots\pr{\gamma_n-\beta_m}.
\end{align*}
So the resultant is homogeneous of degree \(mn\) in the variables \(\beta_i, \gamma_j\).
The resultant is invariant under any permutation of the roots of \(b(x)\), and also under any permutation of the roots of \(c(x)\).
Indeed the resultant is by definition expressed in terms of the coefficients of \(b(x)\) and \(c(x)\), not the roots.
The coefficients are homogeneous polynomials in the roots, elementary symmetric polynomials.
Expanding out the coefficients
\begin{align*}
b(x) &= x^m + b_{m-1} x^{m-1} + \dots + b_0, \\
     &= x^m - e_1(\beta) x^{m-1} + e_2(\beta)x^{m-2} + \dots \pm e_m(\beta),
     \\
c(x) &= x^n + c_{n-1} x^{n-1} + \dots + c_0, \\
     &= x^n - e_1(\gamma) x^{n-1} + e_2(\gamma)x^{n-2} + \dots \pm e_n(\gamma),
\end{align*}
we see that \(b_j\) has degree \(m-j\) in the roots.

Suppose now that we just take any polynomial \(b(x)\) of degree \(m\) with coefficients being abstract variables \(b_j\). 
We will now invent a different concept of \emph{weight}.
Assign each coefficient \(b_j\) the \emph{weight}\define{weight} \(m-j\) and then define the \emph{weight} of a monomial in the \(b_j\) to be  the sum of weights of its factors.
\begin{lemma}
With this notion of weight, every term in the resultant has weight \(mn\).
\end{lemma}
\begin{proof}
By theorem~\vref{theorem:splitting.field}, we can replace our field by some larger field, to arrange that \(b(x)\) and \(c(x)\) split into linear factors, and then the weight of each term in the resultant is clear by expanding the product in terms of roots.
\end{proof}
\begin{proposition}\label{proposition:resultant.degree}
Given any two polynomials \(b(x,y)\) of total degree \(m\) and \(c(x,y)\) of total degree \(n\) over a field, in two variables \(x,y\), either there are at most \(mn\) values of \(y\) for which there is some point \((x,y)\) which satisfies both \(b(x,y)=0\) and \(c(x,y)=0\) (and the resultant in \(x\) vanishes at those values), or \(b(x,y)\) and \(c(x,y)\) have a common factor as polynomials in \(x,y\).
If the polynomials are homogeneous then either they have a homogeneous common factor, or their resultant is homogeneous nonzero of degree \(mn\).
\end{proposition}
\begin{proof}
Think of the polynomials as polynomials in \(x\) with coefficients rational in \(y\):
\[
b(x,y)=\sum_{j+k \le m} b_{jk} x^k y^j=\sum_j b_j(y) x^j,
\]
we see that \(b_j(y)\) has degree at most \(m-j\), exactly the weight of the coefficient \(b_j(y)\) as it enters into the resultant.
Therefore the resultant is a polynomial of degree at most \(mn\) in \(y\).
The resultant vanishes at those values \(y=a\) for which there is a common factor between \(b(x,a)\) and \(c(x,a)\), and in particular if there is a common root it vanishes.
So there are at most \(mn\) such points or the resultant is everywhere zero.
If the resultant vanishes everywhere, then \(b(x,y)\) and \(c(x,y)\) have a common factor with coefficients rational functions of \(y\).
By Gauss's lemma (proposition~\vref{proposition:Gauss.lemma}) they also have a common factor in polynomials in \(x,y\). 

For homogeneous polynomials, each term \(b_j(y)\) has degree exactly \(m-j\), so the resultant either vanishes everywhere or has degree exactly \(mn\).
\end{proof}
\begin{problem}{permutions:rationals}
Prove that, over any field, any symmetric rational function (i.e. permutation invariant) has an expression \(b(e_1,\dots,e_n)/c(e_1,\dots,e_n)\) as a ratio of polynomials with no common nonconstant factor, expressed in the elementary symmetric polynomials, unique up to rescaling numerator and denominator by the same nonzero constant.
\end{problem}
\begin{answer}{permutions:rationals}
If there were two such, cross multiply and apply unique factorisation of polynomials (theorem~\vref{theorem:ufd}).
To see that there is one such, write out the function as \(p(x_1,\dots,x_n)/q(x_1,\dots,x_n)\).
Apply a permutation of the variables, and the solution of problem~\vref{problem:factoring:rationals}, to see that each permutation alters the numerator by a nonzero constant multiple, and the denominator by the same multiple.
Swapping two variables gives some multiple, and repeating that swap gives the same multiple squared, but returns to the original order of the variables.
So any swap either changes sign or leaves both numerator and denominator alone.
Multiple the numerator and denominator by the function \(\prod_{i<j}(t_i - t_j)\) to ensure that the don't change when we transpose, and so they don't change under any permutation.
Apply theorem~\vref{theorem:symmetric.polynomials.algebra}.
\end{answer}
