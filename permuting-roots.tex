\chapter{Permuting roots}
\epigraph[author={Hermann Weyl}, etc={On \'Evariste Galois's letter, written the night before Galois died in a pistol duel. Galois's letter is about permutations of roots of polynomials.}]{This letter, if judged by the novelty and profundity of ideas it contains, is perhaps the most substantial piece of writing in the whole literature of mankind.}\SubIndex{Weyl, Hermann}

\section{Vieta's formulas}

For now, we work over any field.
Which polynomials have which roots?
How are the coefficients related to the roots?
\begin{example}
To get a quadratic polynomial \(p(x)\) to have roots at \(3\) and \(7\), we need it to have \(x-3\) and \(x-7\) as factors.
So it has to be 
\begin{align*}
p(x)
&=
a(x-3)(x-7),
\\
&=
a\pr{x^2-7x-3x+(-3)(-7)},
\\
&=
a\pr{x^2-(3+7)x+3 \cdot 7}.
\end{align*}
\end{example}

A polynomial is \emph{monic}\define{monic polynomial}\define{polynomial!monic} if its leading coefficient is \(1\).
We can make any polynomial monic by dividing off the leading term:
\[
3x^2-4x+1 = 3\pr{x^2-\frac{4}{3}x + \frac{1}{3}}.
\]
The only monic quadratic polynomial \(p(x)\) with roots \(x=3\) and \(x=7\) is
\[
p(x) = x^2-(3+7)x+3 \cdot 7.
\]
The only quadratic monic polynomial \(p(x)\) with roots \(x=r\) and \(x=s\), by the same steps, must be
\[
p(x) = x^2-(r+s)x+rs.
\]
By the same steps, the only cubic monic polynomial \(q(x)\) with roots at \(x=r\), \(x=s\) and \(x=t\) is
\[
q(x)=(x-r)(x-s)(x-t).
\]
If we multiply it all out, we get
\[
q(x)=x^3-(r+s+t)x^2+(rs+rt+st)x-rst.
\]
Ignoring the minus signs, the coefficients are
\begin{align*}
&1 \\
&r+s+t \\
&rs+rt+st \\
&rst
\end{align*}
There is some pattern: 
\begin{enumerate}
\item
There is a minus sign, turning on and off like a light switch, each time we write down a term.
\item
Each coefficient is a sum of products of roots, taken in all possible ways with a fixed number of roots.
\end{enumerate}

\begin{proposition}[Vieta's formula (Viet\'e)]\SubIndex{Viet\'e, Francois}
If a monic polynomial \(p(x)\) splits into a product of linear factors, say
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
then the numbers \(r_1, r_2, \dots, r_n\) are the roots of \(p(x)\), and the coefficients of \(p(x)\), say
\[
p(x) = x^n - a_1 x^{n-1} + \dots \pm  a_{n-1} x \pm a_n,
\]
(with signs switching each term in front of the \(a_j\) coefficients) are computed from the roots by
\begin{align*}
a_1 &= r_1 + r_2 + \dots + r_n, \\
a_2 &= \sum_{i < j} r_i r_j, \\
& \vdotswithin{=} \\
a_i &= \sum_{j_1 < j_2 < \dots < j_i} r_{j_1} r_{j_2} \dots r_{j_i}, \\
& \vdotswithin{=} \\
a_{n-1} &= r_1 r_2 \dots r_{n-1} + r_1 r_2 \dots r_{n-2} r_n + \dots + r_2 r_3 \dots r_n, \\
a_n &= r_1 r_2 \dots r_n.
\end{align*}
\end{proposition}
The \emph{elementary symmetric polynomials} are the polynomials \(e_1, \dots, e_n\)\Notation{ej}{e_j}{elementary symmetric polynomials} of variables \(t_1, t_2, \dots, t_n\), given by
\begin{align*}
e_1\of{t_1,t_2,\dots,t_n} &= t_1 + t_2 + \dots + t_n, \\
e_2\of{t_1,t_2,\dots,t_n} &= t_1 t_2 + t_1 t_3 + \dots + t_{n-1} t_n, \\
                          &= \sum_{i < j} t_i t_j, \\
                          & \vdotswithin{=} \\
e_i\of{t_1,t_2,\dots,t_n} &= \sum_{j_1 < j_2 < \dots < j_i} t_{j_1} t_{j_2} \dots t_{j_i}, \\
                          & \vdotswithin{=} \\
e_{n-1}\of{t_1,t_2,\dots,t_n} &= t_1 t_2 \dots t_{n-1} + t_1 t_2 \dots t_{n-2} t_n + \dots + t_2 t_3 \dots t_n, \\
e_n\of{t_1,t_2,\dots,t_n} &= t_1 t_2 \dots t_n.
\end{align*}
So we can restate our proposition as:
\begin{proposition}[Vieta's formula]\label{proposition:vieta}
If a monic polynomial \(p(x)\) splits into a product of linear factors, say
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
then the numbers \(r_1, r_2, \dots, r_n\) are the roots of \(p(x)\), and the coefficients of \(p(x)\):
\[
p(x) = x^n - e_1 x^{n-1} + \dots (-1)^{n-1}  e_{n-1} x + (-1)^n e_n,
\]
are the elementary symmetric polynomials 
\[
e_i=e_i\of{r_1,r_2,\dots,r_n}
\]
of the roots \(r_1, r_2, \dots, r_n\).
\end{proposition}

\begin{proof}
We can see this immediately for linear polynomials: \(p(x)=x-r\), and we checked it above for quadratic and cubic ones.
It is convenient to rewrite the elementary symmetric polynomials as 
\[
e_i\of{r_1,r_2,\dots,r_n}
=
\sum r_1^{b_1} r_2^{b_2} \dots r_n^{b_n}
\]
where the sum is over all choices of numbers \(b_1, b_2, \dots, b_n\) so that
\begin{enumerate}
\item
each of these \(b_j\) is either \(0\) or \(1\) and
\item
so that altogether 
\[
b_1 + b_2 + b_3 + \dots + b_n = i,
\]
\end{enumerate}
or in other words, ``turn on'' \(i\) of the roots, and turn off the rest, and multiply out the ones that are turned on, and then sum over all choices of which to turn on.

If we expand out the product
\[
p(x)=
\pr{x-r_1}
\pr{x-r_2}
\dots
\pr{x-r_n}
\]
we do so by pick whether to multiply with \(x\) or with \(-r_1\) from the first factor, and then whether to multiply with \(x\) or with \(-r_2\) from the second, and so on, and we add over all of these choices.
Each choice write as \(b_1=0\) if we pick to multiply in the \(x\), but \(b_1=1\) if we pick to multiply in the \(-r_1\), and so on:
\begin{align*}
p(x) 
&= 
\sum_{b_1,\dots,b_n} \pr{-r_1}^{b_1} \pr{-r_2}^{b_2} \dots \pr{-r_n}^{b_n}
x^{\pr{1-b_1} + \pr{1-b_2} + \dots + \pr{1-b_n}},
\\
&=
\sum_j
\sum_{b_1+\dots+b_n=j} \pr{-r_1}^{b_1} \pr{-r_2}^{b_2} \dots \pr{-r_n}^{b_n}
x^{n-j}.
\end{align*}
\end{proof}

\section{Symmetric polynomials}
A polynomial \(f\left(t_1,t_2,\dots,t_n\right)\) is \emph{symmetric}\define{symmetric!polynomial}\define{polynomial!symmetri c} if it is unchanged by permuting the variables \(t_1, t_2, \dots, t_n\).
\begin{example}
\(t_1+t_2+\dots+t_n\) is symmetric.
\end{example}
For any numbers or variables \(t=\pr{t_1, t_2, \dots, t_n}\) let
\[
P_t(x)\defeq\left(x-t_1\right)\left(x-t_2\right) \dots \left(x-t_n\right).
\]
Clearly the roots of \(P_t(x)\) are precisely the entries of the vector
\(t\).
Let \(e(t)\defeq\pr{e_1(t),e_2(t),\dots,e_n(t)}\), so that \(e\) maps vectors to vectors (with entries in our field).

Recall that a field is \emph{algebraically closed} if every polynomial with coefficients in that field splits into a product of linear factors with coefficients in that field.
Recall the fundamental theorem of algebra (theorem~\vref{theorem:FTA}): the field of complex numbers \(\C{}\) is algebraically closed.
\begin{lemma}
For each vector \(c\) with entries in a field, there is a vector \(r\), with entries in a finite degree extension field, so that \(e(r)=c\).
\end{lemma}
\begin{proof}
Let \(r_1, r_2, \dots, r_n\) be the roots of the polynomial
\[
P(x) = x^n - c_1 x^{n-1} + c_2 x^{n-2} + \dots + (-1)^n c_n
\]
in its splitting field.
By proposition~\vref{proposition:vieta}, \(P_r(x)\) has coefficients precisely the same as those of \(P(x)\), i.e. \(P_r(x)=P(x)\).
\end{proof}
\begin{lemma}
Given some variables over a field, if there are more elements of the field than variables, then there is no nontrivial polynomial equation satisfied by the elementary symmetric polynomials.
\end{lemma}
Consequently, any such polynomial relation disappears after some field extension.
\begin{proof}
Any relation \(p(e(t))=0\) between the elementary symmetric polynomials must be satisfied at all points \(c\), so is trivial for any function \(p\).
For a polynomial \(p\), which we don't think of as a function, over a large enough field, the expression \(p(e(t))\) vanishing forces \(p(c)\) to vanish for each \(c\).
Varying any one component of \(c\), \(p(c)\) is a polynomial function of that component with too many roots, so is zero in that component for all other components fixed, so the zero polynomial by induction.
\end{proof}
\begin{lemma}\label{lemma:permuty}
Over any field, the entries of two vectors \(u\) and \(v\) are permutations of one another just when \(e(u)=e(v)\), i.e. just when the elementary symmetric polynomials take the same values at \(u\) and at \(v\).
\end{lemma}
\begin{proof}
The values of the entries of \(e(u),e(v)\) are the coefficients of \(P_u(x),P_v(x)\) whose roots are the entries of \(u,v\).
Clearly the following are equivalent:
\begin{itemize}
\item
The entries of \(u,v\) agree up to order.
\item
The monic polynomials with those roots agree.
\item
\(P_u(x)=P_v(x)\).
\item
\(P_u(x),P_v(x)\) have the same coefficients.
\item
\(e(u)=e(v)\).
\end{itemize}
\end{proof}
\begin{lemma}\label{lemma:auts.are.Sn}
Over any field, a list of rational functions \(f(t)=(f_1(t),\dots,f_n(t))\), with \(t=(t_1,\dots,t_n)\), are precisely the functions \(t=(t_1,\dots,t_n)\) up to reordering just when they satisfy
\[
e(f(t))=e(t).
\]
\end{lemma}
\begin{proof}
If our field is \(k\), apply lemma~\vref{lemma:permuty} over \(k(t_1,\dots,t_n)\).
\end{proof}
We want to see that every symmetric polynomial \(f\of{t_1,t_2,\dots,t_n}\) has the form \(f(t)=h(e(t))\), for a unique polynomial \(h\), and conversely if \(h\) is any polynomial at all, then \(f(t)=h(e(t))\) determines a symmetric polynomial.
We want a recipe to write down each symmetric polynomial in terms of the elemenary symmetric polynomials, to find this mysterious \(h\).
\begin{example}
Take the polynomial 
\[
f=3x^2yz+3xy^2z+3xyz^2+5xy+5xz+5yz.
\]
Clearly the terms with the 5's look like an elementary symmetric polynomial:
\[
f=3x^2yz+3xy^2z+3xyz^2+5e_2.
\]
(We write \(e_2\) to mean the polynomial \(e_2(x,y,z)=xy+xz+yz\), the 2nd elementary symmetric polynomial.)
But what about the terms with the 3's?
Factor them all together as much as we can:
\[
f=3xyz(x+y+z)+5e_2.
\]
Then it is clear:
\[
f=3e_3e_1+5e_2.
\]
\end{example}

If \(a=\pr{a_1,a_2,\dots,a_n}\), write \(t^a\)\Notation{xa}{x^a}{multivariable exponent} to mean
\(t_1^{a_1} t_2^{a_2} \dots t_n^{a_n}\). 
Order terms by ``alphabetical'' order, also called \emph{weight}\define{weight}: order monomials by the order in \(t_1\); if two monomials have the same order in \(t_1\), break the tie by looking at the order in \(t_2\), and so on.
\begin{example}
The term
\[
t_1^5 t_2^3 
\]
has higher weight than any of
\[
t_1^3 t_2^5, t_1^5 t_2^2, 1, \text{ and } t_2^{1000}.
\]
\end{example}
\begin{example}
Showing the highest order term, and writing dots to indicate lower order terms,
\begin{align*}
e_1(t)&=t_1 + \dots, \\
e_2(t)&=t_1t_2 + \dots, \\
& \vdotswithin{=} \\
e_j(t)&=t_1 t_2 \dots t_j + \dots \\
& \vdotswithin{=} \\
e_n(t)&=t_1 t_2 \dots t_n.
\end{align*}
\end{example}

If a symmetric polynomial contains a term, then it also contains every term obtained by permuting the variables:
\[
t_1^3 t_2^5 + t_1^5 t_2^3.
\]
\begin{example}
It is somehow easier to read \(x,y,z\) than \(t_1,t_2,t_3\), so take a symmetric polynomial 
\[
f(x,y,z)=6 x^{16} y^9 z^7 + \dots
\]
where we only write out the highest weight term.
In such notation, 
\begin{align*}
e_1(x,y,z)&=x+y+z=x+\dots, \\
e_2(x,y,z)&=xy+xz+yz=xz + \dots, \\
e_3(x,y,z)&=xyz.
\end{align*}
Imagine a term in \(f\): if the highest power in that term is in \(y\) or \(z\), permute the variables to get that highest power into \(x\), giving a higher weight term in \(f\).
So the highest weight term contains the variables \(x, y, z\), with highest power in \(x\) and successively lower powers in each of \(y,z\).
By the same reasoning, inside the highest weight term, each of \(x,y,z\) appears to at least to the power of the \(z\) variable: in this case, at least to a power of \(7\).
Factor out \(7\) powers of each variable; we underline that factored out part so you can see it:
\[
f=6 x^9 y^2 \underline{\pr{x y z}^7} + \dots
\]
In the remaining factors, there are now no \(z\) variables, and each variable appears at least to a power of \(2\), so we factor out 2 of each:
\[
f=6 x^7 \underline{\pr{xy}^2} \pr{xyz}^7 + \dots
\]
So finally it is clear that \(f\) has the same highest weight as
\(
6 e_1^7 e_2^2 e_3^7.
\)
Hence 
\[
f = 6 e_1^7 e_2^2 e_3^7 + \dots
\]
up to terms of lower weight.
\end{example}

%If we pick integers \(d_1, d_2, \dots, d_n \ge 0\),
%\begin{align*}
%e_1(t)^{d_1} e_2(t)^{d_2} \dots e_n(t)^{d_n}
%&=
%t_1^{d_1} \pr{t_1t_2}^{d_2} \dots \pr{t_1t_2\dots t_n}^{d_n} + \dots,
%\\
%&=
%t_1^{d_1+d_2+\dots+d_n} t_2^{d_2+d_3+\dots+d_n} \dots 
%t_n^{d_n} + \dots.
%\end{align*}
%Write this more neatly as
%\[
%e(t)^d = t^a+\dots
%\]
%where
%\begin{align*}
%a_1&\defeq d_1+d_2+d_3+\dots+d_n, \\ 
%a_2&\defeq d_2+d_3+\dots+d_n, \\
%a_3&\defeq d_3+\dots+d_n
%& \vdotswithin{\defeq} \\
%a_n&\defeq d_n.
%\end{align*}
%
%On the other hand, given a decreasing sequence of nonnegative integers 
%\[
%a=\pr{a_1,a_2,\dots,a_n}
%\]
%calculate \(d\) by
%\begin{align*}
%d_1&\defeq a_1-a_2, \\
%d_2&\defeq a_2-a_3, \\
%& \vdotswithin{\defeq} \\
%d_n&\defeq a_n
%\end{align*}
%and then \(e(t)^d=t^a+\dots\).
%So for each decreasing sequence \(a=\pr{a_1,a_2,\dots,a_n}\), we have \(t^a=e(t)^d+\dots\).

\begin{theorem}\label{theorem:symmetric.polynomials.algebra}
Every symmetric polynomial over any commutative ring has exactly one expression as a polynomial in the elementary symmetric polynomials.
\end{theorem}
\begin{proof}
Write the highest weight term as some coefficient times
\[
t^a=t_1^{a_1} t_2^{a_2} \dots t_n^{a_n}.
\]
If \(a_1\) is smaller than \(a_2\), swap \(t_1\) with \(t_2\) to get a higher weight term, a contradiction, so \(a_1 \ge a_2\).
In the same way, \(a_1 \ge a_2 \ge a_3 \ge \dots \ge a_n\).
Let 
\begin{align*}
d_1&\defeq a_1-a_2,\\
d_2&\defeq a_2-a_3,\\
&\vdotswithin{\defeq}\\
d_{n-1}&\defeq a_{n-1}-a_n,\\
d_n&\defeq a_n.
\end{align*}
Let's search for the highest weight term in \(e^d\):
\begin{align*}
e^d&=
e_1^{d_1}\dots e_n^{d_n},\\
&=(t_1+\dots)^{d_1}(t_1t_2+\dots)^{d_2}\dots (t_1\dots t_n)^{d_n},
\\
&=
t_1^{d_1}(t_1t_2)^{d_2}\dots (t_1\dots t_n)^{d_n} + \dots 
\\
&=
t_1^{a_1-a_2}(t_1t_2)^{a_2-a_3}\dots (t_1\dots t_{n-1})^{a_{n-1}-a_n}(t_1\dots t_n)^{a_n} + \dots,
\\
&=
t_1^{a_1} t_2^{a_2} \dots t_n^{a_n} +\dots
\\
&=
t^a+\dots
\end{align*}
So whatever coefficient of \(t^a\) we find in our given polynomial, subtract that multiple of \(e^d\) from the polynomial, and we reduce the weight.
By induction, we can somehow write the lower weight terms as polynomials in the elementary symmetric polynomials.
\end{proof}
\begin{example}
The sum of squares of two variables is symmetric: 
\[
x^2+y^2=\pr{x+y}^2-2xy.
\] 
To compute out these expressions: \(f(x,y)=x^2+y^2\) has highest term \(x^2\). 
The polynomials \(e_1(x,y)=x+y\) and \(e_2(x,y)=xy\) have highest terms \(x\) and \(xy\).
So we subtract off \(e_1(x,y)^2\) from \(f(x,y)\), and find \(f(x,y)-e_1(x,y)^2=-2xy=-2e_2(x,y)\).
\end{example}

\begin{problem}{permuting.roots:express.in.s}
Express each of the following polynomials as polynomials in the elementary symmetric polynomials:
\begin{enumerate}
\item \(x^3y^3z + x^3yz^3+xy^3z^3\)
\item \(4xyz^3+4xzy^3+4yzx^3\)
\item \(x^4y^4z^4\)
\item \(xyz+x^2y^2z^2\)
\end{enumerate}
\end{problem}
\begin{answer}{permuting.roots:express.in.s}
\begin{enumerate}
\item
\begin{align*}
x^3y^3z + x^3yz^3+xy^3z^3
&=
(xyz)(x^2y^2+x^2z^2+y^2z^2),
\\
&=e_3(x^2y^2+\dots),
\\
&=
e_3(e_2^2+\dots).
\end{align*}
But \(e_2=xy+xz+yz\), so 
\[
e_2^2=x^2y^2+x^2z^2+y^2z^2+2x^2yz+2xy^2z+2xyz^2,
\]
so
\begin{align*}
x^3y^3z + x^3yz^3+xy^3z^3
&=
e_3(e_2^2-[2x^2yz+2xy^2z+2xyz^2]),
\\
&=
e_3(e_2^2-2e_3[x+y+z]),
\\
&=
e_3(e_2^2-2e_3e_1)
\end{align*}
\item \(4xyz^3+4xzy^3+4yzx^3=4e_3(e_1^2-2e_2)\)
\item \(x^4y^4z^4=e_3^4\)
\item \(xyz+x^2y^2z^2=e_3\pr{1+e_3}\)
\end{enumerate}
\end{answer}

\section{Symmetric functions}
A \emph{symmetric function}\define{symmetric function} is a rational function of several variables, invariant under permutation of those variables.
\begin{problem}{permutions:rationals}
Prove that every symmetric function over any field is a ratio of polynomials in the elementary symmetric polynomials.
\end{problem}
\begin{answer}{permutions:rationals}
We will prove that any symmetric function has an expression \(b(e_1,\dots,e_n)/c(e_1,\dots,e_n)\) as a ratio of polynomials with no common nonconstant factor, expressed in the elementary symmetric polynomials, unique up to rescaling numerator and denominator by the same nonzero constant.
If there were two such, cross multiply and apply unique factorisation of polynomials (theorem~\vref{theorem:ufd}).
To see that there is one such, write out the function as \(p(t_1,\dots,t_n)/q(t_1,\dots,t_n)\).
Apply a permutation of the variables, and the solution of problem~\vref{problem:factoring:rationals}, to see that each permutation alters the numerator by a nonzero constant multiple, and the denominator by the same multiple.
Swapping two variables gives some multiple, and repeating that swap gives the same multiple squared, but returns to the original order of the variables.
So any swap either changes sign or leaves both numerator and denominator alone.
Replace our expression for our rational function by
\[
\frac{p(t_1,\dots,t_n)q(t_1,\dots,t_n)}{q(t_1,\dots,t_n)^2}.
\]
So there is an expression in symmetric polynomials.
Factor out any common factors.
Apply theorem~\vref{theorem:symmetric.polynomials.algebra}.
\end{answer}

\section{Sage}
Sage can compute with elementary symmetric polynomials.
It writes them in a strange notation.
The polynomials we have denoted by \(e_3\) sage denotes by \verb!e[3]!.
More strangely, sage denotes \(e_1 e_2^3 e_5^2\) as \verb!e[5,5,2,2,2,1]! which is the same in sage as \verb!e[2]^3*e[1]*e[5]^2!.
You can't write \verb!e[1,2,2,2,5,5]!; you have to write it as \verb!e[5,5,2,2,2,1]!: the indices have to decrease.
Set up the required rings:
\begin{sageblock}
P.<w,x,y,z>=PolynomialRing(QQ)
S=SymmetricFunctions(QQ)
e=S.e()
\end{sageblock}
This creates a ring \(P=\Q{}[w,x,y,z]\), and then lets \(S\) be the ring of symmetric polynomials.
The last (mysterious) line sets up the object \(e\) to be the elementary symmetric polynomials of the ring \(S\).
We can then define a polynomial in our variables:
\begin{sageblock}
f = w^2+x^2+y^2+z^2+w*x+w*y+w*z+x*y+x*z+y*z
e.from_polynomial(f)
\end{sageblock}
which prints out \(\sage{e.from_polynomial(f)}\), the expression of \(f\) in terms of elementary symmetric polynomials.
To expand out a symmetric polynomial into \(w,x,y,z\) variables:
\begin{sageblock}
q = e[2,1]+e[3]
q.expand(4,alphabet=['w','x','y','z'])
\end{sageblock}
prints out 
\[
\sage{q.expand(4,alphabet=['w','x','y','z'])}
\]


\section{Sums of powers}
Define \(p_j(t)=t_1^j+t_2^j+\dots+t_n^j\),\Notation{pj}{p_j}{sum of powers polynomials} the sums of powers.
\begin{lemma}[Isaac Newton\SubIndex{Newton, Isaac}]
The sums of powers are related to the elementary symmetric polynomials by
\begin{align*}
0 &= e_1 - p_1, \\
0 &= 2 \, e_2 - p_1 \, e_1 + p_2, \\ 
& \vdotswithin{=} \\
0 &= k \, e_k - p_1 \, e_{k-1} + p_2 \, e_{k-2} - \dots + (-1)^{k-1} p_{k-1} \, e_1
+ (-1)^k p_k,
\end{align*}
\end{lemma}
Using these equations, we can write the sums of powers in terms of the elementary symmetric polynomials over any commutative ring.
On the other hand, we can only write the elementary symmetric polynomials in terms of the sums of powers over any field of characteristic zero or larger than \(n\).
\begin{proof}
Let's write \(t^{({\ell})}\) for \(t\) with the \({\ell}\)\textsuperscript{th} entry removed, so if \(t\) is a vector with \(n\) entries, then \(t^{({\ell})}\) is a vector with \(n-1\) entries.
\begin{align*}
p_j e_{k-j}
&=
\sum_{\ell} t_{\ell}^j
\sum_{i_1 < i_2 < \dots < i_{k-j}} t_{i_1} t_{i_2} \dots t_{i_{k-j}}
\\
\intertext{Either we can't pull a $t_{\ell}$ factor out of the second sum, or we can:}
&=
\sum_{\ell} t_{\ell}^j
\sum^{i_1, i_2, \dots \ne {\ell}}_{i_1 < i_2 < \dots < i_{k-j}}
t_{i_1} t_{i_2} \dots t_{i_{k-j}}
+
\sum_{\ell} t_{\ell}^{j+1}
\sum^{i_1, i_2, \dots \ne {\ell}}_{i_1 < i_2 < \dots < i_{k-j-1}}
t_{i_1} t_{i_2} \dots t_{i_{k-j-1}}
\\
&=
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{({\ell})}\right)
+
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right).
\end{align*}
Putting in successive terms of our sum,
\begin{align*}
p_j e_{k-j} - p_{j+1} e_{k-j-1}
=&
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{(\ell)}\right)
+
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right)
\\
-&
\sum_{\ell} t_{\ell}^{j+1} e_{k-j-1}\left(t^{({\ell})}\right)
-
\sum_{\ell} t_{\ell}^{j+2} e_{k-j-2}\left(t^{({\ell})}\right)
\\
=&
\sum_{\ell} t_{\ell}^j e_{k-j}\left(t^{({\ell})}\right)
-
\sum_{\ell} t_{\ell}^{j+2} e_{k-j-2}\left(t^{({\ell})}\right).
\end{align*}
Hence the sum collapses to
\begin{align*}
p_1 e_k - p_2 e_{k-1} + \dots + (-1)^{k-1} p_{k-1} e_1
&=
\sum_{\ell} t_{\ell} e_{k-1}\left(t^{({\ell})}\right)
+(-1)^{k-1}
\sum_{\ell} t_{\ell}^k \cdot e_0\left(t^{(\ell)}\right)
\\
&=
k \, e_k + (-1)^{k-1} p_k.
\end{align*}
\end{proof}
\begin{proposition}
Every symmetric polynomial over a field of characteristic zero is a polynomial in the sums of powers.
\end{proposition}
\begin{proof}
We can solve recursively for the sums of powers in terms of the elementary symmetric polynomials and conversely.
\end{proof}

\section{The invariants of a square matrix}\label{pg:linear.invariants}
Over any field, a rational function \(f(A)\) in the entries of a square matrix \(A\), with coefficients in the field, is \emph{invariant}\define{invariant!of square matrix} if \(f\left(FAF^{-1}\right)=f(A)\) for any invertible matrix \(F\) with coefficients in the field.
An invariant is independent of change of basis: if \(T \colon V \to V\) is a linear map on an \(n\)-dimensional vector space, we can define the value \(f(T)\) of any invariant \(f\) of \(n \times n\) matrices, by letting \(f(T)\defeq f(A)\) where \(A\) is the matrix associated to \(T\) in some basis of \(V\).
\begin{example}
The determinant is an invariant of square matrices.
\end{example}
\begin{example}
For any \(n \times n\) matrix \(A\), write
\[
\det(A-\lambda I)=\chi_A(\lambda)=
e_n(A) - e_{n-1}(A) \lambda + e_{n-2}(A) \lambda^2 + \dots + (-1)^n \lambda^n.
\]
The expressions \(e_1(A), e_2(A), \dots, e_n(A)\) are invariants, while \(\chi_A(\lambda)\) is the \emph{characteristic polynomial}\define{characteristic polynomial} of \(A\).
\end{example}
\begin{example}\label{example:Powers}
If we write the trace of a matrix \(A\) as \(\tr{A}\), then the expressions
\[
p_k(A) = \tr\of{A^k}\SubIndex{trace}
\]
are invariant polynomials in the entries of \(A\).
\end{example}
\begin{problem}{SymmetricFunctions:Two}
If \(A\) is diagonal, say
\[
A =
\begin{pmatrix}
t_1 \\
& t_2 \\
& & \ddots \\
& & & t_n
\end{pmatrix},
\]
then prove that \(e_j(A)=e_j\left(t_1,t_2,\dots,t_n\right)\), the elementary symmetric polynomials of the eigenvalues.
\end{problem}
\begin{answer}{SymmetricFunctions:Two}
\begin{align*}
\chi_A{\lambda}
&=
\left(t_1-\lambda\right)\left(t_2-\lambda\right)
\dots \left(t_n-\lambda\right)\\
&=(-1)^n P_{t}\left(\lambda\right)\\
&=
e_n(t) - e_{n-1}(t) \lambda + e_{n-2}(t) \lambda^2 + \dots + (-1)^n \lambda^n.
\end{align*}
\end{answer}
\begin{problem}{permuting.roots:diag.symmetric}
Generalize the previous exercise to \(A\) diagonalizable.
\end{problem}
\begin{problem}{permuting.roots:inverse.Cramer}
Prove that the entries of \(A^{-1}\) are rational functions of the entries of the square matrix \(A\), over any field.
\end{problem}
\begin{answer}{permuting.roots:inverse.Cramer}
By theorem~\vref{theorem:adjugate}, \(A^{-1}=(\det A)^{-1}\adj{A}\), so
\[
A^{-1}_{ij} = \frac{(-1)^{i+j}\det (A \text{ with row \(j\) and column \(i\) deleted})}{\det A},
\]
rational functions of the entries of \(A\).
\end{answer}

\begin{example}
Let \(\Delta_A\) be the discriminant of the characteristic polynomial of \(A\): \(\Delta_A \defeq \Delta_{\chi_A}\).
The map \(A \mapsto \Delta_A\) is also a polynomial invariant of \(A\), as the coefficients of the characteristic polynomial are.
\end{example}
\begin{problem}{SymmetricFunctions:discriminant}
Take a square matrix \(A\).
Suppose that the characteristic polynomial of \(A\) splits into linear factors.
Prove that \(\Delta_A = 0\) just when \(A\) has an eigenvalue of multiplicity two or more.
\end{problem}
\begin{theorem}
Every invariant polynomial (or invariant rational function) of square matrices, over any field with more elements than the number of columns of the matrices, has exactly one expression as a polynomial (rational function) in the elementary symmetric polynomials of the eigenvalues. 
\end{theorem}
We can replace the elementary symmetric polynomials of the eigenvalues by the sums of powers of the eigenvalues, over any field of characteristic zero or larger than the number of columns.
\begin{proof}
Take an invariant polynomial \(f(A)\).
(The same argument will work for an invariant rational function.)
Every invariant polynomial \(f(A)\) determines an invariant polynomial \(f(t)\)
by setting
\[
A =
\begin{pmatrix}
t_1 \\
& t_2 \\
& & \ddots \\
& & & t_n
\end{pmatrix}.
\]
Taking \(F\) any permutation matrix, invariance tells us that \(f\left(FAF^{-1}\right)=f(A)\). 
But \(f\left(FAF^{-1}\right)\) is given by applying the associated permutation to the entries of \(t\).
Therefore \(f(t)\) is a symmetric polynomial. 
Therefore \(f(t)=h(e(t))\), for some polynomial \(h\); so \(f(A)=h(e(A))\) for  diagonal matrices. 
Replace \(f\) by \(f(A)-h(e(A))\) to arrange that \(f(A)=0\) on all diagonal matrices \(A\).
By invariance, \(f(A)=0\) on all diagonalizable matrices. 

The equation \(f(FAF^{-1})=f(A)\) holds for all \(F\) over our field.
Imagine that \(F\) has abstract variables as entries.
The difference \(f(FAF^{-1})-f(A)\) is a rational function in the entries of \(F\).
Its numerator vanishes for any choice of values of those abstract variables, and so vanishes.
Hence \(f(FAF^{-1})=f(A)\) for \(F\) with abstract variable entries.
In particular, if our field lies in a larger field, then \(f\) remains invariant over the larger field, because we can plug into the entries of \(F\) the values in the larger field.

Recall that a matrix has an eigenvector for each eigenvalue, so if there are \(n\) distinct eigenvalues, then \(A\) is diagonalizable.
This occurs just when the characteristic polynomial \(\chi_A(\lambda)\) splits into distinct linear factors, and so \(\Delta_A \ne 0\).
Conversely, if \(\Delta_A \ne 0\) and \(\chi_A(\lambda)\) splits into linear factors, then they are distinct, and so \(f(A)=0\).
By theorem~\vref{theorem:splitting.field}, we can sit our field into a larger field in which the characteristic polynomial \(\chi_A(\lambda)\) splits into linear factors, and therefore if \(\Delta_A\ne 0\) then \(f(A)=0\).

Pick any matrix \(A_0\) whose eigenvalues are all distinct.
In particular, \(\Delta_{A_0} \ne 0\).
Take any matrix \(A_1\).
For an abstract variable \(t\), let \(A_t \defeq (1-t)A_0+tA_1\).
Since \(\Delta_{A_t}\) is a polynomial, not vanishing at \(t=0\), it is a nonzero polynomial in \(t\).
So \(\Delta_{A_t}\ne 0\) except for finitely many \(t\).
Hence \(f(A_t)=0\) except for finitely many \(t\).
But \(f(A)\) is a polynomial, so vanishes for all \(t\).
\end{proof}
\begin{example}
The function \(f(A)=e_j\left(\left|\lambda_1\right|,\left|\lambda_2\right|,\dots,\left|\lambda_n\right|\right)\), where the matrix \(A\), with complex number entries, has eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_n\), is a continuous invariant function of a real matrix \(A\), and is \emph{not} a polynomial in \(\lambda_1, \lambda_2, \dots, \lambda_n\).
\end{example}

\section{Resultants and permutations}
\begin{example}
If \(b(x,y)=x^2+y^2-1\) and \(c(x,y)=x^2+(y-1)^2-1\), how can we find the points \((x,y)\) at which both \(b(x,y)=0\) and \(c(x,y)=0\)? 
\begin{center}
\inputinexample{degenerate-resultant-0}
\end{center}
Think of \(b(x,y)\) and \(c(x,y)\) as polynomials in \(x\), with coefficients rational functions of \(y\).
Then the resultant \(r(y)\) of \(b(x,y),c(x,y)\) is a rational function of \(y\).
Compute it: \(r(y)=(2y-1)^2\) vanishes at \(y=1/2\).
So there is a common factor there:
\[
b(x,1/2)=c(x,1/2)=\pr{x-\frac{\sqrt{3}}{2}}\pr{x+\frac{\sqrt{3}}{2}}.
\]
The common factor came from two common roots.
\end{example}
\begin{problem}{permuting.roots:solve.by.res}
Working over \(k=\R{}\), let \(b(x,y)\defeq xy^2+x^{10}y+1\) and \(c(x,y)\defeq -xy^2+2x^{13}y+1\).
Let \(B\) be the curve with equation \(0=b(x,y)\) and \(C\) the one with equation \(0=c(x,y)\).
Use a resultant to find the points \((x,y)\) of the plane where the curves meet.
\end{problem}
\begin{answer}{permuting.roots:solve.by.res}
The resultant in \(y\) involves determinant of smaller matrix than that in \(x\), because the degrees are smaller:
\begin{align*}
r(y)
&=
\det
\begin{pmatrix}
1&0&1&0\\
x^{10}&1&2x^{13}&1\\
x&x^{10}&-x&2x^{13}\\
0&x&0X-x
\end{pmatrix},
\\
&=
4x^{27}-4x^{24} + x^{21},
\\
&=
x^{21}(2x^3-1)^2,
\end{align*}
so
\[
x=0\text{ or }x=\frac{1}{\sqrt[3]{2}}.
\]
If we let \(x=0\) we have no solutions.
(This ``false solution'' is caused by dropping of degree of both \(b(x,y)\) and \(c(x,y)\), which invalidates the method we are using, since the resultant of lower order polynomials has a different expression.)
If \(x=1/\sqrt[3]{2}\), look at \(0=b+c\) to see
\[
y=-\frac{2}{x^{10}+2x^{13}}=\frac{1}{\sqrt[3]{2}^2}.
\]
\end{answer}
In chapter~\ref{chapter:resultants}, we saw that for any two polynomials split into linear factors
\begin{align*}
b(x) &= \pr{x-\beta_1} \pr{x-\beta_2} \dots \pr{x-\beta_m}, \\
c(x) &= \pr{x-\gamma_1} \pr{x-\gamma_2} \dots \pr{x-\gamma_n}, \\
\end{align*}
the resultant is
\begin{align*}
\resultant{b}{c} 
=
\pr{\gamma_1-\beta_1}\pr{\gamma_1-\beta_2}\dots\pr{\gamma_n-\beta_m}.
\end{align*}
So the resultant is homogeneous of degree \(mn\) in the variables \(\beta_i, \gamma_j\).
The resultant is invariant under any permutation of the roots of \(b(x)\), and also under any permutation of the roots of \(c(x)\).
Indeed the resultant is by definition expressed in terms of the coefficients of \(b(x)\) and \(c(x)\), not the roots.
The coefficients are homogeneous polynomials in the roots, elementary symmetric polynomials.
Expanding out the coefficients
\begin{align*}
b(x) &= x^m + b_{m-1} x^{m-1} + \dots + b_0, \\
     &= x^m - e_1(\beta) x^{m-1} + e_2(\beta)x^{m-2} + \dots \pm e_m(\beta),
     \\
c(x) &= x^n + c_{n-1} x^{n-1} + \dots + c_0, \\
     &= x^n - e_1(\gamma) x^{n-1} + e_2(\gamma)x^{n-2} + \dots \pm e_n(\gamma),
\end{align*}
we see that \(b_j\) has degree \(m-j\) in the roots.

Suppose now that we just take any polynomial \(b(x)\) of degree \(m\) with coefficients being abstract variables \(b_j\). 
We will now invent a different concept of \emph{weight}.
Assign each coefficient \(b_j\) the \emph{weight}\define{weight} \(m-j\) and then define the \emph{weight} of a monomial in the \(b_j\) to be  the sum of weights of its factors.
\begin{lemma}
With this notion of weight, every term in the resultant has weight \(mn\).
\end{lemma}
\begin{proof}
By theorem~\vref{theorem:splitting.field}, we can replace our field by some larger field, to arrange that \(b(x)\) and \(c(x)\) split into linear factors.
We assign each root a weight of one; this gives the correct weight to each coefficient \(b_j,c_j\).
The weight of each term in the resultant is just its degree in the roots, so \(mn\) as above.
\end{proof}
\begin{proposition}\label{proposition:resultant.degree}
Over any field, given any two polynomials \(b(x,y),c(x,y)\) of degrees \(m,n\) in two variables \(x,y\), either 
\begin{itemize}
\item
\begin{itemize}
\item
their resultant in \(x\) is a nonzero polynomial in \(y\) of degree at most \(mn\) and 
\item
vanishes at those values of \(y\) for which there is some point \((x,y)\) which satisfies both \(b(x,y)=0\) and \(c(x,y)=0\) and so
\item
the number of such values is at most the degree of the resultant,
\end{itemize}
\item
or 
\begin{itemize}
\item
their resultant in \(x\) or in \(y\) is the zero polynomial and
\item
\(b(x,y)\) and \(c(x,y)\) have a common factor as polynomials in \(x,y\).
\end{itemize}
\end{itemize}
If the polynomials are homogeneous then either their resultant is nonzero of degree exactly \(mn\) or they have a homogeneous common factor.
\end{proposition}
\begin{proof}
Think of the polynomials as polynomials in \(x\) with coefficients rational in \(y\):
\[
b(x,y)=\sum_{j+k \le m} b_{jk} x^k y^j=\sum_j b_j(y) x^j,
\]
we see that \(b_j(y)\) has degree at most \(m-j\), exactly the weight of the coefficient \(b_j(y)\) as it enters into the resultant.
Therefore the resultant is a polynomial of degree at most \(mn\) in \(y\).
The resultant vanishes at those values \(y=a\) for which there is a common factor between \(b(x,a)\) and \(c(x,a)\), and in particular if there is a common root it vanishes.
So there are at most \(mn\) such points or the resultant is the zero polynomial.
If the resultant is the zero polynomial, then \(b(x,y)\) and \(c(x,y)\) have a common factor with coefficients rational functions of \(y\).
By Gauss's lemma (proposition~\vref{proposition:Gauss.lemma}) they also have a common factor in polynomials in \(x,y\). 

For homogeneous polynomials, each term \(b_j(y)\) has degree exactly \(m-j\), so the resultant either vanishes everywhere or has degree exactly \(mn\).
\end{proof}
\begin{problem}{permuting.roots:plane.algebraic.curves}
Over every field, find the resultant of \(b(x,y)=x^2+y^2\) and \(c(x,y)=x^2+y^4+1\), thought of as polynomials in the variable \(x\), with coefficients being rational functions of \(y\).
What does this resultant tell you?
Suppose that \(B\) is the set of points \((x,y)\) where \(b(x,y)=0\) and \(C\) the set of points where \(c(x,y)=0\) for \(x\) and \(y\) in that field.
How many points of intersection can \(B\) and \(C\) have?
\end{problem}
\begin{answer}{permuting.roots:plane.algebraic.curves}
\[
\det
\begin{pmatrix}
y^2&0&y^4+1&0\\
0&y^2&0&y^4+1\\
1&0&1&0\\
0&1&0&1
\end{pmatrix}
=(y^4-y^2+1)^2
\]
vanishes just at values of \(y=y_0\) for which there are common factors in \(b(x,y_0),c(x,y_0)\) as polynomials in \(x\), and in particular drops if there is common root.
There are at most \(4\) roots \(y=y_0\) of \(y^4-y^2+1=0\) in any field.
At each of these there are at most \(4\) roots of \(b(x,y)\), so at most \(4\cdot 4=16\) intersection points in total.
To be more precise, in a field not of characteristic \(2\),
\[
(x,y)=
(\pm i \omega, \pm \omega),
\]
where 
\[
\omega=
\sqrt{
-\frac{1\pm\sqrt{3}i}{2}
}
\]
if the relevant square roots exist in the field.
For example, none of these exist over \(k=\Q{}\) or \(k=\R{}\), but all do over \(k=\C{}\).
In a field of characteristic \(2\), we can let \(u=y^2\), and find that \(u^2=1\), so \(u=1\), a double root, and then \(y^2=1\) so \(y=1\), a double root, and then \(x^2+1=0\), so \(x^2=1\), but also \(x^2+1+1=0\) so \(x^2=0\), a contradiction, so there are no intersection points of \(B\) and \(C\) if our field \(k\) has characteristic \(2\).
\end{answer}
