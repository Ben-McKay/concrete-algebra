\chapter{The tangent line}\label{chapter:tangent.line}
\epigraph[author={Karl Barth}, source={The Humanity of God (1960), p. 42.}]{What expressions we used---in part taken over and in part newly invented! above all, the famous `wholly other' breaking in upon us `perpendicularly from above,' the not less famous `infinite qualitative distinction' between God and man, the vacuum, the mathematical point, and the tangent in which alone they must meet.}\SubIndex{Barth, Karl}
\section{Lines crossing a curve}
\begin{proposition}\label{proposition:lines.crossing}
Pick an irreducible plane algebraic curve \(C\) of degree \(d\) and a point \(p\) of the plane not lying on \(C\).
Among all of the lines through \(p\), every one of them strikes \(C\) in exactly \(d\) distinct points, except for at most \(d(d-1)\) lines.
\end{proposition}
\begin{proof}
First assume that \(C\) is irreducible.
Arrange that the point \(p\) lies at the origin in an affine plane \(z=1\), so that the lines through \(p\) are those of the form \(y=mx\) or \(x=0\).
Also arrange that \(C\) is not entirely on the line at infinity, so contains some finite points.
If \(C\) has equation \(C=(0=c(x,y))\), the intersections lie on \(c(x,mx)=0\), a polynomial of degree at most \(d\) in \(x\).
This polynomial is not the zero polynomial, because the origin does not lie on \(C\).
Therefore the number of intersections is \(d\), counted with multiplicities.
The multiplicities will all be \(1\) except at values of \(m\) where the discriminant (as a polynomial in \(x\)) of \(c(x,mx)=0\) vanishes.
The discriminant is a polynomial of degree at most \(d(d-1)\) in \(m\).

Suppose that the discriminant vanishes as a polynomial in \(m\).
It vanishes as a rational function of \(m\).
Think of \(c(x,mx)\) as a polynomial in \(x\) with coefficients in the field of rational functions of \(m\).
This polynomial has a zero of higher multiplicity: \(c(x,mx)=(x-f(m))^2 Q(x,m)\).
Think of \(c(x,y)\) as a polynomial in \(x\) with coefficients rational functions of \(y\).
Replacing \(m\) by \(y/x\), \(c(x,y)\) is reducible as such a polynomial, and so by the Gauss' lemma \(c(x,y)\) is reducible as a polynomial in \(x,y\), a contradiction.
So the discriminant is a nonzero polynomial, and if it is constant we can homogenise and dehomogenise to get it to be nonconstant.
So it has the required degree.
\end{proof}
Looking at our curve in the affine plane, by setting \(z=1\), the curve ``homogenises'' into its cone in \(k^3\), by homogenising the equation. 
The tangent line in the affine plane \(z=1\) homogenises into a plane through the origin, the tangent plane to the cone.
When we set \(x=1\) or \(y=1\), or more generally we look at how our homogenised cone surface passes through some plane not containing the origin, the tangent plane to the cone becomes a tangent line to the resulting curve.
Therefore we can speak of a \emph{tangent line}\define{tangent line} to a plane algebraic curve in the projective plane, by which we mean the projective line which corresponds to the tangent plane to the cone.

To calculate that tangent line, we can either operate directly in some affine plane, say \(x=1\), and just differentiate implicitly as usual, or we can homogenise, and then differentiate the equation of the surface via partial derivatives.
\begin{example}
The curve \(y^2=x^2+x^3\) has tangent line given by differentiating implicitly:
\[
2yy' = 2x + 3x^2,
\]
so that at the point \((x,y)=(3,6)\), we have
\[
2(6)y'=2(3)+3(3)^2,
\]
i.e. the slope of the tangent line is
\[
y'=\frac{11}{4},
\]
giving the tangent line
\[
y-6=\frac{11}{4}(x-3),
\]
at the point \((x,y)=(3,6)\).
\end{example}
\begin{example}
For the same curve, we could instead homogenise to the cone
\[
y^2z=x^2z+x^3,
\]
and then treat any one of the variables as a function of the other two, say treating \(z=z(x,y)\), so take partial derivatives:
\begin{align*}
y^2\pderiv{z}{x} &= 2xz+x^2\pderiv{z}{x} + 3x^2, \\
2yz+y^2\pderiv{z}{y} &= x^2\pderiv{z}{y}.
\end{align*}
We are interested in the point \((x,y)=(3,6)\) in the affine plane \(z=1\), so the point \((x,y,z)=(3,6,1)\).
We plug in these values to our equations:
\begin{align*}
6^2\pderiv{z}{x} &= 2(3)(1)+3^2\pderiv{z}{x} + 3(3)^2, \\
2(6)(1)+6^2\pderiv{z}{y} &= 3^2\pderiv{z}{y}.
\end{align*}
Solve these to find
\begin{align*}
\pderiv{z}{x} &= \frac{11}{9}, \\
\pderiv{z}{y} &= -\frac{4}{9},
\end{align*}
giving an equation of the tangent plane:
\[
z-1 = \frac{11}{9}(x-3)-\frac{4}{9}(y-6),
\]
which simplifies to
\[
z = \frac{11}{9}x -\frac{4}{9} y.
\]
This is the same solution, because along the affine plane \(z=1\) it gives
\[
1=\frac{11}{9}x -\frac{4}{9} y,
\]
which, solving for \(y\), is
\[
y-6=\frac{11}{4}(x-3),
\]
exactly the same solution we had above.
\end{example}
\begin{example}
At the point \((x,y)=(0,0)\), the process above fails to yield a tangent line to the curve \(y^2=x^2+x^3\) since differentiating implicitly:
\[
2yy' = 2x + 3x^2,
\]
and plugging in the point \((x,y)=(3,6)\), we have \(0=0\), no equation at all.
In general, our procedure yields a tangent line to an irreducible curve precisely at the points \((x,y)\) where the polynomial \(p(x,y)\) in the equation \(p(x,y)=0\) of the curve has at least one nonzero partial derivative, so that we can solve for \(y'(x)\) or for \(x'(y)\) implicitly.
In this example, 
\inputinexample{cubic-curve-1}
there are clearly \(2\) different lines tangent to the curve.
We find these by taking the homogeneous terms of lowest degree in \(x,y\), \(y^2=x^2\), dropping the \(x^3\) because it has higher order, and then factoring into \(y=\pm x\).
We can justify this over the real numbers (or the complex numbers), by saying that our equation \(y^2=x^2+x^3\) has solutions \(y=\pm \sqrt{x^2+x^3} \cong \pm x^2\), asymptotically as \(x \to 0\). 
\end{example}
\begin{example}
The curve \(y=x^n\) in the affine plane \(z=1\) has no singularities in that affine plane, if \(n \ge 1\) is an integer.
In the affine plane \(x=1\) it becomes \(z^{n-1}y=1\) which has singularities just when 
\[
z^{n-1}y=1 \text{ and } z^{n-1}=0 \text{ and } (n-1)z^{n-2} y = 0,
\]
which never happen simultaneously.
Therefore over any field, the curve \(y=x^n\) is regular for every \(n=1, 2, \dots\).
In particular, there are regular curves of all degrees.
\end{example}
\begin{theorem}\label{theorem:tangent.intersection}
The tangent line to a projective plane algebraic curve at a regular point is the unique line that has intersection multiplicity \(2\) or more at that point.
More generally, the intersection multiplicity of a line \(\ell\) and a curve \(C=(0=f)\) at a point \(p\) is the multiplicity of the zero of \(p\) as a zero of the restriction of \(f\) to \(\ell\).
\end{theorem}
\begin{proof}
We can arrange that our coordinates axes form a generic triangle as usual, and that our intersection point is at \((x,y)=\pr{1,1}\).
We can assume that our projective plane algebraic curve is
\[
0=f(x,y)=\sum_j f_j(x)y^j.
\]
We can arrange that our line is \(y=1\).
The intersection multiplicity is the multiplicity of \(x=1\) as a zero of the resultant
\begin{align*}
&\det
\begin{pmatrix}
-1 &      &        &        &     & f_0(x) \\
1  &   -1 &        &        &     & f_1(x) \\
   &    1 & -1     &        &     & f_2(x) \\
   &      & \ddots & \ddots &     & \vdots \\
   &      &        &    1   &  -1 & f_{n-1}(x) \\  
   &      &        &        &  1  & f_n(x)
\end{pmatrix},
\\
\intertext{and we add each row to the next}
&=
\det
\begin{pmatrix}
-1 &      &        &        &          & f_0(x) \\
   &   -1 &        &        &          & f_0(x) + f_1(x) \\
   &      &    -1  &        &          & f_0(x) + f_1(x) + f_2(x) \\
   &      &        & \ddots &          & \vdots  \\
   &      &        &        &       -1 & f_0(x) + f_1(x) + \dots +  f_{n-1}(x) \\  
   &      &        &        &            &  f_0(x) + f_1(x) + \dots + f_n(x)
\end{pmatrix},
\\
&=(-1)^n \pr{f_0(x)+f_1(x)+\dots+f_n(x)},
\\
&=
(-1)^n f(x,1).
\end{align*}
So the intersection multiplicity of the line and the curve is the multiplicity of vanishing of \(f(x,1)\) at \(x=1\).
Since \(f(1,1)=1\), the intersection multiplicity is positive, and is two or more just when 
\[
\left.\frac{d}{dx}\right|_{x=1} f(x,1)=0,
\]
i.e. just when \(y=1\) is a tangent line.
\end{proof}

\begin{theorem}\label{theorem:multiplicity.submultiplicative}
Denote by \(\order{p}{C}\) the order of a projective algebraic plane curve \(C\) at a point \(p\).
Then for any point \(p\) and projective algebraic plane curves \(C, D\):
\[
\order{p}{C}\order{p}{D} \le \multiplicity{p}{C}{D},
\]
with equality just when no line defined over any extension field is tangent to both curves at \(p\).
In particular, the order \(\order{p}{C}\) is the intersection multiplicity \(\multiplicity{p}{C}{\ell}\) for any line \(\ell\) through \(p\) which is not tangent to \(C\) at \(p\).
\end{theorem}
\begin{proof}
If \(C\) or \(D\) is a line, our result is precisely theorem~\vref{theorem:tangent.intersection}.
If either \(C\) or \(D\) is reducible, our result follows from induction on the degrees of the components and adding up multiplicities.

After perhaps replacing our field by a finite degree extension, pick a generic triangle for the two curves, and arrange that its vertices are \((0,0), (0,\infty), (\infty,0)\) as usual.
It is easier to compute tangent lines if we shift the axes by adding constants to \(x\) and \(y\) to arrange that the vertices are at \((-1,-1), (-1,\infty), (\infty,-1)\).
We can then arrange that our point \(p\) is \((x,y)=(0,0)\).
Write the equations of our curves as \(C=(f=0)\) and \(D=(g=0)\).
Then \(\multiplicity{p}{C}{D}\) is the order of \(x=0\) as a zero of the resultant \(r(x)=\resultant{f}{g}(x)\) in \(y\).
Let \(r\defeq \order{p}{C}\), \(s\defeq \order{p}{D}\). 
The lowest order homogeneous polynomial terms have order equal to the order of the point on the curve, so we can write out
\begin{align*}
f(x,y)&= f_0(x) x^r + f_1(x) x^{r-1} y + \dots + f_r(x) y^r \\
      & \quad + f_{r+1}(x) y^{r+1} + \dots + f_m(x) y^m, \\
g(x,y)&= g_0(x) x^s + g_1(x) x^{s-1} y + \dots + g_s(x) y^s \\
      & \quad + g_{s+1}(x) y^{s+1} + \dots + g_n(x) y^n.
\end{align*}
The tangent lines are the zero lines of the homogeneous terms of lowest order in \(x,y\):
\begin{align*}
0&= f_0(0) x^r + f_1(0) x^{r-1} y + \dots + f_r(0) y^r, \\
0&= g_0(0) x^s + g_1(0) x^{s-1} y + \dots + g_s(0) y^s.
\end{align*}
We can assume that our axes are chosen so that the horizontal and vertical axes are not tangent to either curve at the origin, i.e. to arrange that neither of these equations are satisfied by \((x,y)=(1,0)\) or by \((x,y)=(0,1)\), i.e. none of
\[
f_0(0), g_0(0), f_r(0), g_s(0)
\]
vanishes.
Since these tangent line equations are homogeneous, to find tangent lines we can restrict to \(y=1\):
\begin{align*}
0&= f_0(0) x^r + f_1(0) x^{r-1} + \dots + f_r(0), \\
0&= g_0(0) x^s + g_1(0) x^{s-1} + \dots + g_s(0).
\end{align*}
The resultant of these two polynomials doesn't vanish just when these polynomials have no common factor, i.e. just when the curves \(C\) and \(D\) have no common tangent line; called this resultant \(R_1\).

The point \((x,y)=(0,0)\) is by assumption an intersection point of \(C\) and \(D\).
We can arrange, making our triangle generic enough, again perhaps using a field extension, that the lines \(x=0\) and \(y=0\) do not contain any other intersection points of \(C\) and \(D\) other than \((x,y)=(0,0)\).
This is precisely saying that our polynomials
\begin{align*}
f(x,y)&= f_0(x) x^r + f_1(x) x^{r-1} y + \dots + f_r(x) y^r \\
      & \quad + f_{r+1}(x) y^{r+1} + \dots + f_m(x) y^m, \\
g(x,y)&= g_0(x) x^s + g_1(x) x^{s-1} y + \dots + g_s(x) y^s \\
      & \quad + g_{s+1}(x) y^{s+1} + \dots + g_n(x) y^n.
\end{align*}
do not both vanish when we substitute \(x=0\), except at \(y=0\), and vice versa.
Plugging in \(x=0\):
\begin{align*}
f(0,y)&= f_r(0) y^r + f_{r+1}(0) y^{r+1} + \dots + f_m(0) y^m, \\
g(0,y)&= g_s(0) y^s + g_{s+1}(0) y^{s+1} + \dots + g_n(0) y^n.
\end{align*}
Dividing out common factors of \(y\):
\begin{align*}
\frac{f(0,y)}{y^r}&= f_r(0) + f_{r+1}(0) y + \dots + f_m(0) y^{m-r}, \\
\frac{g(0,y)}{y^s}&= g_s(0) + g_{s+1}(0) y + \dots + g_n(0) y^{n-s}.
\end{align*}
These are both polynomials and don't vanish at \(y=0\), since \(f\) and \(g\) vanish exactly to orders \(r\) and \(s\) at \(y=0\).
These two polynomials also can't both vanish anywhere else, since there are no common roots of \(f\) and \(g\) on \(x=0\) except at \(y=0\).
Hence the resultant of these two doesn't vanish; call this resultant \(R_2\).

We next claim that \(\resultant{f}{g}(x)=x^{rs} R_1 R_2 + \dots\) has lowest order term given by these two resultants.
Before we prove our claim, note that if our claim is true, then \(\multiplicity{p}{C}{D}\) is given by the order of this resultant, so is at least \(rs\), proving the theorem.
Moreover this order is \(rs\) just when \(R_1 \ne 0\) (since we know that \(R_2 \ne 0\)), i.e. just when there are no common tangent lines.

We are left to justify this claim, i.e. to compute out the lowest order term of
\[
\resultant{f}{g}(x)
=
\det
\begin{pmatrix}
f_0(x) x^r     &        & g_0(x) x^s     &  \\
f_1(x) x^{r-1} & \ddots & g_1(x) x^{s-1} & \ddots \\
\vdots         & \ddots & \vdots           & \ddots \\
f_m(x)         & \ddots & g_n(x) & \ddots \\
0              & \ddots & 0      & \ddots
\end{pmatrix}.
\]
The lowest order term in \(x\) is the same as the lowest order term of what happens when we plug \(x=0\) in all of the polynomials:
\[
\det
\begin{pmatrix}
f_0(0) x^r     &        & g_0(0) x^s     &  \\
f_1(0) x^{r-1} & \ddots & g_1(0) x^{s-1} & \ddots \\
\vdots         & \ddots & \vdots           & \ddots \\
f_m(0)         & \ddots & g_n(0) & \ddots \\
0              & \ddots & 0      & \ddots
\end{pmatrix}.
\]
So it suffices to prove our result for the special case that all \(f_j(x)\) and \(g_j(x)\) are constants, say:
\begin{align*}
f(x,y)&= a_0 x^r + a_1 x^{r-1} y + \dots + a_r y^r \\
      & \quad + a_{r+1} y^{r+1} + \dots + a_m y^m, \\
g(x,y)&= b_0 x^s + b_1 x^{s-1} y + \dots + b_s y^s \\
      & \quad + b_{s+1} y^{s+1} + \dots + b_n y^n.
\end{align*}
Then the resultant is
\[
\resultant{f}{g}(x)
=
\det
\begin{pmatrix}
a_0 x^r        &        & b_0 x^s     &  \\
a_1 x^{r-1}    & \ddots & b_1 x^{s-1} & \ddots \\
\vdots         & \ddots & \vdots        & \ddots \\
a_m            & \ddots & b_n         & \ddots \\
0              & \ddots & 0           & \ddots
\end{pmatrix}.
\]
We need to prove that this is \(x^{rs} R_1 R_2 + \dots\).
Write out
\[
R_1
=
\det
\begin{pmatrix}
a_0             &          & b_0         &  \\
a_1            & \ddots    & b_1         & \ddots \\
\vdots         & \ddots     & \vdots       & \ddots \\
a_r            & \ddots    & b_s         & \ddots \\
0              & \ddots    & 0           & \ddots
\end{pmatrix}
\]
and
\[
R_2
=
\det
\begin{pmatrix}
a_r              &        & b_s          &  \\
a_{r+1}          & \ddots & b_{s+1}      & \ddots \\
\vdots           & \ddots  & \vdots        & \ddots \\
a_m              & \ddots & b_n          & \ddots \\
0                & \ddots & 0            & \ddots
\end{pmatrix}.
\]

Let's try a simple case: if \(r=2, s=1, m=3, n=2\), the resultant is
\[
\resultant{f}{g}(x)
=
\det
\begin{pmatrix}
a_0 x^2        & 0       & b_0 x       &  0       & 0 \\
a_1 x          & a_0 x^2 & b_1         &  b_0 x   & 0 \\
a_2            & a_1 x   & b_2         &  b_1     & b_0 x \\
a_3            & a_2     & 0           &  b_2     & b_1 \\
0              & a_3     & 0           &  0       & b_2
\end{pmatrix}.
\]
Sadly, when we expand out this determinant, we get a huge mess:
\begin{align*}
\resultant{f}{g}(x)
&=
x^2 R_1 R_2
\\
& 
\qquad
+ x^3 (a_{3}^{2} b_{0}^{3} 
- 2 \, a_{1} a_{3} b_{0}^{2} b_{2} 
+ 3 \, a_{0} a_{3} b_{0} b_{1} b_{2} 
+ a_{1}^{2} b_{0} b_{2}^{2} 
- 2 \, a_{0} a_{2} b_{0} b_{2}^{2} 
- a_{0} a_{1} b_{1} b_{2}^{2} )
\\
&
\qquad
+ x^4 a_{0}^{2} b_{2}^{3}
\end{align*}
where
\[
R_1 = a_{2} b_{0}^{2} - a_{1} b_{0} b_{1} + a_{0} b_{1}^{2}
\]
and
\[
R_2 = a_{2} b_{2}-a_{3} b_{1}.
\]
Nevertheless, the lowest term (as expected) is \(\resultant{f}{g}(x) = x^2 R_1 R_2 + \dots\).

%Then the resultant is
%\[
%\resultant{f}{g}(x)
%=
%\det
%\begin{pmatrix}
%a_0 x^r        &        & b_0 x^s     &  \\
%a_1 x^{r-1}    & \ddots & b_1 x^{s-1} & \ddots \\
%\vdots         & \dots & \dots        & \dots \\
%a_m            & \ddots & b_n         & \ddots \\
%0              & \ddots & 0           & \ddots
%\end{pmatrix}.
%\]
%Multiply the first column by \(x^s\), the second by \(x^{s-1}\), and so on down to \(x^1\).
%Multiply column \(n+1\) by \(x^r\), the next by \(x^{r-1}\), and so on down to \(x^1\).
%Now divide the first row by \(x^{r+s}\), the second by \(x^{r+s-1}\), and so on down to \(x^1\).
%In all, you change the determinant by a bunch of \(x\) factors, but the coefficient of the lowest term doesn't change.
%To be precise, we multiply the determinant by
%\[
%x^{s+(s-1)+\dots+1} x^{r+(r-1)+\dots+1}
%\]
%and then divide by
%\[
%x^{(r+s)+(r+s-1)+\dots+1}.
%\]
%The reader can check by induction that the result of the multiplications and divisions all put together is a division by\(x^{rs}\).
%
%In our example, all of this work gives
%\[
%\resultant{f}{g}(x)
%=
%x^{rs}
%\det
%\begin{pmatrix}
%a_0            & 0       & b_0         &  0       & 0 \\
%a_1            & a_0     & b_1         &  b_0     & 0 \\
%a_2            & a_1     & b_2  x      &  b_1     & b_0 \\
%a_3 x          & a_2     & 0           &  b_2 x   & b_1 \\
%0              & a_3     & 0           &  0       & b_2
%\end{pmatrix}.
%\]
%In general, for any size of matrix, another induction proof shows that each of our divisions by powers of \(x\) always results in a matrix with polynomial entries, since the multiplications put in high enough powers in every entry.
%Our claim is that the constant term in the determinant of this matrix is \(R_1 R_2\).
%The constant term doesn't change if we set \(x=0\):
%\[
%\resultant{f}{g}(x)
%=
%x^{rs}
%\det
%\begin{pmatrix}
%a_0            & 0       & b_0         &  0       & 0 \\
%a_1            & a_0     & b_1         &  b_0     & 0 \\
%a_2            & a_1     & 0           &  b_1     & b_0 \\
%0              & a_2     & 0           &  0       & b_1 \\
%0              & a_3     & 0           &  0       & b_2
%\end{pmatrix} + \dots
%\]
%Shift columns over to the right to arrange a block matrix:
%\begin{align*}
%\resultant{f}{g}(x)
%&=
%x^{rs}
%\det
%\begin{pmatrix}
%a_0            & b_0         &  0       & 0       & 0 \\
%a_1            & b_1         &  b_0     & a_0     & 0 \\
%a_2            & 0           &  b_1     & a_1     & b_0 \\
%0              & 0           &  0       & a_2     & b_1 \\
%0              & 0           &  0       & a_3     & b_2
%\end{pmatrix} + \dots
%\\
%&=x^{rs}R_1 R_2 + \dots.
%\end{align*}
%
Returning to the general problem: luckily, we don't really need to compute any of these resultants at all.
Suppose that \(r \le s\).
After rescaling \(f\), \(x\) and \(y\) we can assume that \(a_r=1\) and that \(a_m=1\).
Every term in \(g-b_s x^{s-r} f\) contains a factor of \(y\), so write it as
\(g-b_s x^{s-r} f = y \tilde{g}\).
We will see that replacing \(g\) by \(\tilde{g}\), the three resultants we have to calculate then all compute the same values.
By problem~\vref{problem:resultants:add.stuff}:
\begin{align*}
\resultant{f(x,y)}{g(x,y)}(x)
&=
\resultant{f(x,y)}{y\tilde{g}(x,y)}(x),
\\
&=
\resultant{f(x,y)}{y}(x)\resultant{f(x,y)}{\tilde{g}(x,y)}(x),
\\
&=
f(x,0)\resultant{f(x,y)}{\tilde{g}(x,y)}(x),
\\
&=
x^r\resultant{f(x,y)}{\tilde{g}(x,y)}(x)
\end{align*}
while, if we let \(F\) be the lowest order part of \(f\), 
\(G\) the lowest order part of \(g\), and \(\tilde{G}\) the lowest order part of \(\tilde{g}\),
\[
R_1=\resultant{F(x,1)}{G(x,1)}=\resultant{F(x,1)}{\tilde{G}(x,1)}
\]
and
\[
R_2
=
\resultant{\frac{f(0,y)}{y^r}}{\frac{y\tilde{g}(0,y)}{y^s}}
=
\resultant{\frac{f(0,y)}{y^r}}{\frac{\tilde{g}(0,y)}{y^{s-1}}}.
\]
Hence our claim is true for \(f,g\) just when it is true for \(f,\tilde{g}\).
Similarly, if \(r < s\), swap the roles of \(f\) and \(g\) and repeat.
Apply induction on \(r\) and \(s\), driving at least one of them to become smaller at each step, with the other staying the same.
The induction stops when one of them is zero, and \(C\) and \(D\) cease to intersect at \(p\).
\end{proof}
\begin{problem}{tangents:conics}
What are the possible intersection multiplicities at a point of two smooth conics, over a field not of characteristic \(2\)?
\end{problem}
\begin{answer}{tangents:conics}
The point might not belong to the conics, giving zero.
Smooth curves are irreducible.
The two conics might agree on a component, but being irreducible, they must then be equal since they are irreducible, and then all of their points have intersection multiplicity \(\infty\).
There is a unique tangent line at each point, by smoothness.
The order of each point is the number of tangent lines: \(1\).
By theorem~\vref{theorem:multiplicity.submultiplicative}, the intersection multiplicity is at least \(1\) at any intersection point, with equality just when the tangents are distinct.
For example, two conics with distinct tangents at the origin: \(y=x^2\) and \(x=y^2\).
The tangents (drop higher order terms): \(y=0\), \(x=0\). 
Resultant
\begin{align*}
r(x)
&=
\det
\begin{pmatrix}
-x^2&0&-x\\
1&-x^2&0\\
0&1&1
\end{pmatrix},
\\
&=
-x^2
\det
\begin{pmatrix}
-x^2&0\\
1&1
\end{pmatrix}
-x
\det
\begin{pmatrix}
1&-x^2\\
0&1
\end{pmatrix},
\\
&=
-x^2(-x^2)-x(1),
\\
&=
-x^4-x,
\\
&=-x(x^3+1).
\end{align*}
The resultant vanishes at \(x=0\) and at the three roots of \(x^3+1=0\).
At the origin, the intersection multiplicity is \(1\).
Finally, suppose we face two conics tangent at some point.
By our classification of conics, we can assume that our first curve, after projective transformation, is \(y=x^2\).
Our second has to intersect at the origin with the same tangent line.
Writing its equation as
\[
0=ax^2+bxy+cy^2+dx+ey+f,
\]
note that passing through the origin forces precisely \(f=0\).
To have tangent \(y=0\), we need lowest order terms to be \(d=0\), \(e\ne 0\).
Rescale to get \(e=1\):
\[
0=ax^2+(bx+1)y+cy^2.
\]
Suppose that \(c\ne 0\).
Now take resultant for our equations \(0=y-x^2\) and this one above:
\begin{align*}
r(x)
&=
\det
\begin{pmatrix}
-x^2&0&ax^2\\
1&-x^2&bx+1\\
0&1&c
\end{pmatrix},
\\
&=
-x^2
\begin{pmatrix}
-x^2&bx+1\\
1&c
\end{pmatrix}
+ax^2
\begin{pmatrix}
1&-x^2\\
0&1
\end{pmatrix},
\\
&=
-x^2(-cx^2-bx-1)+ax^2,
\\
&=
cx^4+bx^3+(a+1)x^2,
\\
&=
x^2(cx^2+bx+(a+1)).
\end{align*}
So the intersection multiplicity at \(x=0\) is \(2\) unless \(a=-1\), i.e.
\[
y=x^2-bxy-cy^2,
\]
for which the intersection multiplicity is \(3\).
If \(c=0\),
\[
0=ax^2+(bx+1)y,
\]
the resultant is
\begin{align*}
r(x)
&=
\det
\begin{pmatrix}
-x^2&ax^2\\
1&bx+1
\end{pmatrix},
\\
&=
-x^2(bx+1)-ax^2,
\\
&=
-x^2(bx+1+a).
\end{align*}
So again the intersection multiplicity at \(x=0\) is \(2\), unless \(a=-1\),
\[
y=x^2-bxy.
\]
for which the intersection multiplicity is \(3\).
\end{answer}
\begin{problem}{tangents:cubic}
Prove that a cubic curve is either smooth or has one singular point.
\end{problem}
\begin{answer}{tangents:cubic}
If a cubic curve \(C\) has two singular points \(p_0,p_1\), then the line \(\ell\) between them has \(\order{p}{C}\le \multiplicity{p}{C}{\ell}\), equality just when \(\ell\) is not tangent to \(C\) at \(p\), for each of the points \(p=p_0,p_1\).
To be singular \(\order{p}{C}\ge 2\), so adding up over \(p=p_0,p_1\), the intersection number of \(C\) and \(L\) is at least \(4\), contradicting B\'ezout's theorem.
(We have seen, or can check by differentiating, that the symmetric cubic curve 
\[
x^3+y^3+1=3axy
\]
over any field of characteristic not \(3\), with \(a^3\ne 1\), is smooth, so there are cubic curves with no singular point over any such field.)
\end{answer}

